---
title: "RL-Diagnóstico"
author: "José Luis Manzanarees Rivera"
date: 2019-02-05T21:11:11-05:00
categories: ["R"]
tags: ["Regresión Lineal", "MCO", "regression"]
mathjax : true
menu:
  main:
    name: RL-Diagnóstico
    weight: 8
---

```{r set-global-options, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(eval = TRUE, 
                      echo = TRUE, 
                      cache = FALSE,
                      include = TRUE,
                      collapse = FALSE,
                      dependson = NULL,
                      engine = "R", # Chunks will always have R code, unless noted
                      error = FALSE)   


```



```{r silent-packages, echo = FALSE, eval = TRUE, message=FALSE, include = FALSE}
library(reticulate)
library(tidyverse)
library(wooldridge)
library(ISLR)
library(olsrr)
library(lmtest)
library(foreign)
```


## Análisis de los supuestos y pruebas de diagnóstico del modelo.


### I  Heterocedsticidad.

Uno de los supuestos principales del Modelo de Regresión Lineal **MRL** es que los  errores tiene la misma varianza, lo que se conoce como **homocedasticidad**, $$E(\epsilon|x)=E(\epsilon)=0$$  $$var(\epsilon|x)=\sigma^2$$ 

Cuando el supuesto no se cumple tenemos el problema denominado **Heterocedasticidad**. 

¿Cuáles son las consecuencias del problema de Heterocedasticidad?

Los parámetros estimados ya no se consideran **BLUE**  (Best **L**inear **U**nbiased **E**stimators  supuestos del teorema Gauss-Markov). Los errores estándard estimados son incorrectos, lo que afecta la construcción de los intervalos de confianza.

**Ya no son válidas las pruebas de hipótesis** sobre los parámetros, los estadísticos **t** y **F** ya no son  válidos.

Note que **este problema no causa que los estimados sean seasgados (*unbiased*), sin embargo, **este problema se traduce en que los parámetros ya no son los de  menor varianza** entre los estimados insesgados (la B de *Best*  en BLUE ya no se cumple, es decir hay otro estimador posible con menor varianza).
  
El primer paso, es detectar el problema, para esta tarea usaremos la prueba Breusch-Pagan   en dos especificaciones cuyos resultados son equivalentes:

+ Breusch-Pagan: Una prueba que detecta el problema de heterocedasticidad en un modelo de regresión  usando una distribución Pearson ($\chi^2$) (al observar si la varianza de los errores depende de los valores que toman las variables independientes).

La prueba plantea como $H_0: Homocedasticidad$ vs. $Ha: Heterocedasticidad$





#### Ejemplo 1

Analizaremos el modelo que relaciona esperanza de vida con *gdp* y tasa de mortalidad infantil para 198 paises.

+ Primero estimamos el modelo. 

+ Hacemos una diagnóstico gráfico de los residuales para detectar el patrón consistente con el problema de heterocedasticidad.

+ Enseguida aplicamos la prueba **Breusch-Pagan** para hacer una valoración formal. 

##### Heterocedasticidad Diagnóstico gráfico.

Graficar Rediduales vs $\hat y$  en un modelo múltiple o bien residuales vs.   variable independiente 


![](/img/heteroc.jpg)

Note el patrón creciente en los residuales (las lineas azules muestran la tenencia en las dispersión de los residuales) síntoma característico del problema de heterocedasticidad. 

##### a) Scatter Plot de residuales contra cada regresor.

Si el supuesto se cumple tendremos una gráfica con observaciones dispersas sin ningún patrón específico. Indicando residuales no correlacionados con los regresores. 

A continuación observe 2 violaciones de los supuestos del modelo de regresión lineal que deben atenderse para obtener parámetros válidos, útiles para hacer inferencia. Panel a) relación no lineal, Panel b) Heterocedasticidad.

![](/img/resid.jpg)

Note en la gráfica b)  se observa evidencia de heterocedasticidad, mientras en la gráfica  a) se observa evidencia de una relación no lineal.  Esta puede resolverse con la trasformación de los regresores (inclusión de término cuadrático,  transformación Box-Cox,etc,.)



**Ejemplo**  Obtención de residuales para diagnóstico gráfico. Considere el ejemplo del modelo sobre relación de años de esperanza de vida y características poblacionales para una muestra de 194 paises.   


```{r, ehco=TRUE, message=FALSE}

library(estimatr)

nt<-readRDS("Nations2a.rds")%>%
  mutate(region=fct_reorder(region,life))%>%
  select(life,gdp, chldmort)
  
nt<-na.omit(nt)

model<-lm(life~gdp+chldmort, data=nt)
summary(model)
```

```{r, echo=TRUE}
nt$predicted <- predict(model)   
nt$residuals <- residuals(model) 

ggplot(nt, aes(chldmort, life)) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  geom_segment(aes(xend = chldmort, yend = predicted), alpha = .2) +
geom_point(aes(color = abs(residuals), size = abs(residuals)))+
  scale_color_continuous(low = "black", high = "red") +
  guides(color = FALSE, size = FALSE) +
  theme_bw()
```

**Método alternativo**. Directamente es posible obtener el diagnóstico gráfico de residuales posterior a la estimación del modelo. Dos gráficas son de interés 


```{r, ehco=FALSE, message=FALSE}

library(estimatr)

nt<-readRDS("Nations2a.rds")%>%
  mutate(region=fct_reorder(region,life))

model<-lm(life~gdp+chldmort, data=nt)
summary(model)


plot(model, which=c(1,1))
```


#### Gráfica  Residuals vs. Fitted (detección de heterocedasticidad)

Residuales vs. $\hat y$.

Una distribución aleatoria de los residuales alrededor de la linea de tendencia (linea roja) indica homocedasticidad (linea roja recta). 

Adicionalmente esta gráfica nos permite detectar si la relación no es lineal entre variable dependiente e independiente.(ej. una linea roja con forma de parábola).




##### Gráfica   Scale-Location (detección de heterocedasticidad).

*Scale-location* o *spread location plot* que mide la dispersión de los residuales y permite detectar así el problema de **heterocedasticidad**.

Nuevamente una linea recta con residuales dispersos de manera aleatoria es una indicación de que se cumple la condicion de varianza constante y no hay problema de  heterocedasticidad.  

En el caso particular el patrón observado presenta una dispersón ligeramente creciente consistente con el problema de heterocedasticidad.


```{r, ehco=FALSE, message=FALSE}

library(estimatr)

nt<-readRDS("Nations2a.rds")%>%
  mutate(region=fct_reorder(region,life))

model<-lm(life~gdp+chldmort, data=nt)
summary(model)

plot(model, which=c(1,3))
```

A continuación la estimación de la prueba formal Breusch - Pagan para detectar heterocedasticidad.  recuerde que la prueba tiene $H0: varianza constante$  $Ha: Heterocedasticidad$ 

```{r, echo=FALSE}
# Una forma usando olsrr library.
library(olsrr)

nt<-readRDS("Nations2a.rds")%>%
  mutate(region=fct_reorder(region,life))

model<-lm(life~gdp+chldmort, data=nt)
summary(model)


model<-lm(life~gdp+chldmort, data=nt)

ols_test_breusch_pagan(model)

## Alternativa usando lmtest library Si los regresores no siguen una distribución normal, con el Koenker's studentized test statistc.
bptest(model,varformula =  ~fitted.values(model))
```
  
Concluimos en este caso que se viola el supuesto de varianza contante en los errores (p-value<0.05), es decir rechazamos $H0:\sigma^2$ constante. (en ambos métodos *p-value* <.05, ej $p-value_{bptest} = 0.0152$), $p-value_{lmtest}0.0003$


#### ¿Cómo atender el problema de Heterocedasticidad?

Corregir este problema es un asunto empírico que depende en última instancia del problema que estamos analizando.

No obstante el punto fundamental para resolver este problema es identificar con la causa, (i.e la fuente de la variación no constante). Lograr lo anterior implica observar por ejemplo el rango de variación de los regresores. Variables con RI grandes pueden estar vinculadas con la causa.


En este mismo sentido es posible por ejemplo usar una unidad de medida alternativa para la variable de interés por ejemplo en lugar de usar datos en niveles con unidades originales se puede considerar el uso de la variable en términos per cápita, en tasa por cada 100 mil  o porcentaje, dependiendo del escenario de análisis.  

Una posible solución es aplicar una transformación logarítmica a la variable dependiente.

Un segundo enfoque es estimar el modelo de regresión ponderada. (*weighted least squares*).



### Diagnóstico sobre los residuales: Normalidad (Shapiro-Wilk), outliers (Cook´s D).

Un supuesto de partida de los residuales es que se distribuyen a corde a la **normal** (Normalidad en los residuales) con media cero varianza constante $\sigma^2$.


Utilizamos la  el qq plot para validar el cumplimiento del supesto de normalidad de $\epsilon$.

Si los residuales se distribuyen a corde a la normal, se observa una distribución a lo largo de la diagonal.

![](/img/qqplotN.jpg)

Este método nos permite detectar desviaciones del supuesto de distribución normal en los errores especialmente contrastar la simetria de la distribución. 

Presenta los cuantiles de la **distribución normal** contra la distribución de los datos estimados. Note que una distribución normal sigue un patrón a lo largo de la diagonal.   



#### Gráfica  QQ norm.


```{r, ehco=FALSE, message=FALSE}

library("car")

nt<-readRDS("Nations2a.rds")%>%
  mutate(region=fct_reorder(region,life))%>%
  select(life,gdp, chldmort)
  nt<-na.omit(nt)

model<-lm(life~gdp+chldmort, data=nt)
nt$residuals <- residuals(model) 
qqPlot(nt$residuals)



```



En este caso observa una deviación que indica una distribución cercana a la normal si bien con datos concentrados hacia las colas (*fat tails*).

Importante considerar la transformación de los datos para atender el problema cuando no se tiene normalidad en los residuales.

Un procedimiento común es la transformación **Box-Cox**. Básicamente buscamos transformar la variable dependiente utilizando un exponente $\lambda$ consistente con el  supuesto de normalidad. 

La siguiente especificación indica el proceso para determinar el $\lambda$ óptimo a corde con el procedimiento de máxima verosimilitud propuesto por **Box-Cox** $$y^ \lambda=x \beta +\epsilon$$ $$\Bigg\{\frac{y^{\lambda} -1}{\lambda}, \lambda \neq0 \\
log (y) , \lambda =0 $$

```{r,echo=TRUE, message=FALSE, warning=FALSE}

## La implementación en *R* a continuación:
 
library(MASS)

nt<-readRDS("Nations2a.rds")%>%
mutate(region=fct_reorder(region,life))

model<-lm(life~gdp+chldmort, data=nt)


r<-residuals(model)

shapiro.test(r)

bc=boxcox(model,lambda=seq(-3,3))


lambda=bc$x[which(bc$y==max(bc$y))]

lambda

## Una vez que estimamos el parámetro lambda, realizamos la 
## transformación con : var_Trans_box = (y ^ lambda - 1)/lambda

```

Note la aplicación de la prueba estadística de **Shapiro-Wilk** que tiene como **H0: Normalidad**. 

Generalmente las pruebas de hipótesis que conocemos se construyen de tal forma que buscamos rechazar la hipótesis nula, sin embargo en este caso la prueba tiene como **H0: normalidad**, por lo que un **p-value<0.5** implican rechazar **normalidad**. 

Entonces con la prueba **Shapiro-Wilk**, no rechazar H0, (ej. p-values  grandes) de hecho se traduce en un buen escenario ya que es razonable asumir que los terminos de  error se distribuyen a corde a la normal.


En este caso tenemos un *p-value*=4.359e-10. Rechazamos **H0**, la distribución de los datos no es acorde con la normal.
 
Para el resultado de la transformación note que tenemos un $\lambda=1.54$ usualmente se considera el valor entero inmediato en este caso 2.


Importante el procedimiento de transformación  no garantiza totalmente obtener datos con distribución normal, especialmente si los datos presentan al grado de sesgo.   

Conclusión, en este caso particular tenemos tambipen influencia severa de **outliers**. No importa que transformación usemos el tema de ouliers (obs 3, 25, 43 y 45) resulta un "obstáculo".

En este caso es conveniente atender el tema de outliers y observaciones con infleuncia notable (*high leverage obs*). A continuación abordamos este tema. 


#### Residuals leverage y  outliers.

Es importante detectar observaciones que tienen influencia importante en la estimación, estas se denominan observaciones con *Hight leverage* 

Así mismo es importante controlar por la presencia de  **oultilers**, puntos convalores  elevados en la predicción dado el valor de ${x_i}$.  

Los  valores inusualmente altos de  **x** (*High leverage obs*), relativo al resto de observaciones  son datos que pudieran tener una influencia especialmente importante en el modelo. 

El enfoque gráfico  es  útil para controlar por obsevaciones que pudieran incluso indicar algún problema en la forma en que se capturó la información. 

Note: Contraste entre obs con gran  **Leverage**  (influencia) vs. **Outliers** (obs para las que el valor de $\hat y$ es relativamente grande  dado el valor de **x**)


![](/img/lev.jpg)

El procedimiento para la detección en R  implica el cálculo del estadístico conocido como distancia *Cook*, (propuesto por R. Dennis Cook in 1977) básicamente este indicador permite   detectar aquellas observaciones con *high leverage*.  La especificación para el estadístico que indica el leverage $h_i=\frac{1}{n}+\frac{(x_i-\bar x)^2}{\sum_{i=1}^n(x_i-\bar x)^2}$ se ubica en el rango [1/n ,1]. 

El *leverage* promedio es siempre igual a $(p+1)/n$ de tal modo que si una observación tiene un estadístico que excede  $(p+1)/n$ entonces tenemos evidencia para suponer  *high leverage*  una observación con elevada influencia.

Una segunda herramienta que es de utilidad para detectar observaciones atípicas  *outliers* en la variable dependiente es la proyección de los residuales *Studentized* (SR).

Los **SR** (Studentized Residuals), son los residiuales divididos por la su  desviación estandar $SR=e/\sqrt{e}$


Note que en este caso  (**outliers**), estamos tratando con el valor que toma la variable dependiente (las $\hat y$). Mientras en el caso de los *high leverage* points nos referimos únicamente a las observaciones de la variable independiente **x_i**.

Cuando el valor del **SR** supera 3, existe evidencia de la presencia de *outliers*.  La selección del valor de referencia 3 derivado de la distribución de los datos en relación al número de desviaciones estándard (recordar la aplicación de la prueba t-student).

Así mismo  en la versión estandarizada *Standarized Residuals* los residuales divididos por la desviación estándar estimada (El **treshold** recomendado es 2 para este indicador). (ej. $95\%$ de los datos contenidos a 2 desviaciones estándard.).

Note la única diferencia entre el indicador *studentize Residuals* y *Standarized Residuals* es que el primero calcula el modelo una vez que se eliminan las observaciones consideradas outliers. Ambos informan el mismo diagnostico indentificando los **outiliers**.  

Y recuerde que controlar por estas  observaciones pude ser importante para los resultados del modelo a estimar. En última instacia la decisión sobre quitarlos o no es una cuestión empírica qu dependerá del problema analizado.



A continuación el estimado de  a) *Cook´s distance*, b) Studentized y c) standarized residuals  y  la gráfica conjunta *Leverage diagnóstics* para mostrar observaciones con high *leverage* y *outliers* 

```{r,echo=FALSE}
nt<-readRDS("Nations2a.rds")%>%
  mutate(region=fct_reorder(region,life))

model<-lm(life~gdp+chldmort, data=nt)

par(mfrow=c(2,2))
ols_plot_cooksd_bar(model)
ols_plot_resid_stud(model)
ols_plot_resid_stand(model)
ols_plot_resid_lev(model)
```

La **solución** es remover estas observaciones y reestimar el modelo.


[Lectura Recomendada. ISL Pag.  97-99](https://drive.google.com/file/d/1zaKc1XWvELsUgZrS32QdpHoM4-w0y7Qx/view?usp=sharing)


[Revisar](https://cran.r-project.org/web/packages/olsrr/vignettes/influence_measures.html)


### Multicolinealidad y su  detección.  El indicador: Variance Inflation Factor  VIF. 

Este problema se presenta cuando dos o más variables estan relacionadas entre sí.

Cuando el supuesto de independencia de las variables independientes $x_1, x_2,...,x_p$ no se cumple tenemos el problema de **colinearidad**

Escenarios que generan Multicolinealidad son: 

+ La inclusión de $P$  variables dummies en lugar de $p-1$. Recuerde que el modelo lineal ya incluye un término intercepto ej. $\beta_0$. 
Con un término intercepto, una vez que se han definido $p-1$ variables binarias (dummies) el valor de la $p_{-esima}$variable ya es conocido y puede considerarse redundante si se incluyen $p$ variables binarias.

+ La inclusión de variables que estan altamente correlacionadas entre sí. La gráfica siguiente ilustra este aspecto. Panel a) Dos variables sin correlación, panel b) dos variables altamente correlacionadas (correlación  positiva).

![](/img/multicoli.jpg)

**Detección:** Observar la matriz de correlación de las variables independientes. Un par de variables con  elevada correlación en valor absoluto indica evidencia de la presencia  de colinealidad en el modelo de regresión lineal.

Un problema asociado es la presencia de colinealidad entre un grupo de variables, (tres ó mas variables) denominado **Multicolinealidad**. En este caso la información que aportan las variables resulta redundante.  Para detectar este problema en lugar de observar la matriz de correlación, es necesario estimar el indicador  **VIF** (*Variance Inflation Factor*). 

El valor más pequeño posible es **1** indicando total ausencia de multicolinealidad ya que el **VIF** es el cociente entre la varianza del $\hat \beta$ cuando se estima el modelo con el conjunto de regresores,  divido por la varianza de $\hat \beta$ cuando se estima el modelo únicamente con el parámetro.

Como una regla general un valor de **VIF** entre 5 y 10 indica problemas de multicolinealidad.

A continuación un **ejemplo** con el modelo que relaciona el precio de las viviendas con una serie de 14 características, incluidos atributos sobre calidad ambiental para 506 observaciones  (census tract). Fuente de base de datos  Harrison, D. and Rubinfeld, D.L. Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81–102.

```{r,echo=FALSE, warning=FALSE, message = FALSE}

library(car)

data("Boston", package = "MASS")
names(Boston)

modelo1<- lm(medv ~., data = Boston)
summary(modelo1)

vif(modelo1)
```
Note el valor  elevado de VIF para la variable *Tax* (monto del impuesto en tasa por cada 10,000 del valor total).  

**Solución**

En el caso de colinealidad entre dos variables la información que aporta  una de las variables involucradas en el problema de colinealidad es redundante, por lo que una solución es eliminar una de las variables con alta correlación.  

Una segunda aproximación para resolver el problema es incluir una variable que combine las variables correlacionadas (por ejemplo con un índice compuesto).

Para antender el problema de multicolinealidad encontrado en el modelo sobre el valor de vivienda debido a la variable *tax*, estimamos el modelo sin esta variable y evaluamos nuevamente el indicador VIF.

```{r,echo=FALSE}
library(car)
data("Boston", package = "MASS")
names(Boston)

model2<- lm(medv ~.-tax, data = Boston)

summary(model2)

vif(model2)
```
Note que el desempeño del modelo en cuanto al estadístico de bondad de ajuste no se ve afectado. modelo2 $R^2=735$ vs. modelo1 $R^2=74$


### Autocorrelación

Note que cuando no se cumple el supuesto de **errores independientes** tenemos el problema de **autocorrelación** Este problema implica que los errores standard **SE** estan subestimados lo que causa que los intervalos de confianza sean más estrechos de lo que deberían ser.

Por ejemplo, recordemos que un intervalo de confianza del $95\%$ indica que existe una probabilidad del $95\%$ de que el parámetro se encuentra en ese rango, sin embargo con el problema de autocorrelación la probabilidad exprezada por un IC al $95\%$ de hecho podria ser menor.

Recordemos además que los *p-values* asociados al *t-statistic* para la prueba de hipótesis dependen también de los **SE** por lo que, estos también serán menores a lo que en realidad deberían ser. Lo que puede traducirse en hacer conclusiones equivocadas sobre la significancia de los $\hat \beta$.

En síntesis, cuando los errores estan correlacionados tendremos un escenario en el que el nivel de confianza en nuestros parámetros está comprometido.

El problema de autocorrelación es común en estructuras de datos como **series de tiempo**, si bien en datos de corte transversal también puede presentarse.

Situaciones que involucran experimentos con personas que tienen la misma dieta, personas que pertenecen a la misma familia o bien que tienen exposición a los mismos factores de riesgo son suceptibles de presentar el problema de autocorrelación en lso residuales.  

Una prueba estándar para detectar autocorrelación es el cálculo del estadístico **Durbin-Watson**  este plantea como **H0:** no autocorrelación y un estadístico de prueba $DW\approx2$. 

**Note** $DW\approx2$ indica que **no hay auto correlación**.

En el siguiente ejemplo consideramos el estudio sobre cambio climático con datos sobre las temperaturas registradas para la región Ártica (variable dependiente  **tempN**: Mide la anomalia o desviación del patrón de temperatura media en un periodo de tiempo dado. La unidad de medida es grados. La variable independiente es el año.

[Fuente](https://www.ncdc.noaa.gov/monitoring-references/faq/anomalies.php)). La base contiene 33 observaciones para el periodo entre  1979-2011. 

##### Ejemplo 1

```{r,echo=FALSE}

library(lmtest)
artico<-read.dta("Arctic9.dta")

modelo<-lm(tempN~year,data=artico)
  
  summary(modelo)

dwtest(modelo)
```

El modelo indica que la variable independiente **año** es estadísticamente significativa y un incremento de un año se asocia con un incremento promedio de .058°C o bien .58°C por década en la temperatura de la región Ártica, (el dato estimado de NASA es .16°C/década en el mismo perido).

Ahora bien, una vez que realizamos la prueba **Durbin-Watson** para detectar autocorrelación sobre los residuales, notamos que de hecho no se rechaza la hipotesis nula $H0: Autocorrelacion=0$ una señal positiva para la validéz de las pruebas de hipótesis sobre los parámetros estimados.

La siguiente gráfica permite contrastar el escenario de autocorrelación entre residuales para tres diferentes grados de correlación $\rho$ en una serie de residuales hipotética.

Notablemente el panel c) que describe  residuales con alto grado de correlación $\rho=.9$ Muestra un patrón de tendencia acentuada entre observaciones adyacentes.   

![](/img/correl.jpg)

#### Elección del modelo más adecuado. Criterios de información Akaike AIC y Bayesiano BIC.

En la estructura de regresión lineal múltiple, la inclusión de nuevas variables explicativas en ocaciones puede resultar en modelos con información redundante como hemos visto al tocar el tema de Multicolinealidad, o bien pueden dejarse fuera importantes variabels explicativas. 


En términos generales el principio de parsimonia implica *ceteris paribus* (*other thing being equal*) que un modelo simple debe ser usado sobre uno de mayor complicación, (si con un modelo sencillo se explica el fenómeno de interés, no necesito crear un modelo con demasiadas variables que al final del día no me aporta más allá de lo necesario para enteder el fenómeno de estudio).

Una vez que hemos validado el cumplimiento de los suspuestos del modelo una manera de seleccionar el modelo adecuado es aplicar un criterio para validar el equilibrio del modelo en términos del principio de parsimonia. 


El criterio de información de Akaike, *AIC* desarrollado por el estadístico Japonés  Hirotugu Akaike, es una herramenta útil para este propósito este penaliza la inclusión de variables explicativas en el modelo. Su expresión es: AIC=2P+nlog(RSS/n): Con **P** como el número de regresores en el modelo y  **n** el tamaño de muestra.  

Note que el **AIC** penaliza la inclusión de *k* variables adicionales en 2k. 

El objetivo es mínimizar el AIC. El modelo con el menor **AIC** es preferible.

Existen otras variantes del AIC  como el **BIC**. En el mismo sentido el *BIC* Bayesian Information criterion, busca contrastar el modelo más adecuado. Tal como en el caso anterior el modelo de menor **BIC** es considerado el más adecuado de entre un conjunto de especificaciones alternativas.

La metodología  **stewise regression** consiste en elegir la especificación del modelo con la inclusión de variables que aportan información  bajo el principio de parsimonia considerando la minimización del **AIC**. Así la técnica permite la eliminación de variables que  cuya aportación a la capacidad explicativa del  modelo no es indispensable. 

##### Ejemplo 2 

A continuación se presenta la aplicación del criterio de información Akaike **AIC** para el modelo sobre precio de la vivienda en función de sus caracterisitcas incluidos aspectos ambientales.  El objetivo es determinar el modelo que minimiza el **AIC**. 

```{r, echo=FALSE, warning=FALSE,message=FALSE}

library(MASS)

data("Boston", package = "MASS")
names(Boston)

modelo1<- lm(medv ~. - tax, data = Boston)

step_lm <- stepAIC(modelo1, direction="both")
summary(step_lm)
```




#### Conclusión:

El proceso de diagnóstico post estimación es muy importante, ya que nos permite detectar problemas de especificación que se pueden resolver antes de avanzar con la fase de inferencia. Tal vez sea necesario aplicar transformaciones a los datos o determinar si realmente tenemos una relación lineal entre la variable dependiente y los regresores.


### Actividad.

1.-Observe la siguiente gráfica que relaciona los residuales de un modelo de regresión lineal con los valores de predicción de la variable dependiente $\hat y$. 


```{r,echo=FALSE, warning=FALSE,message=FALSE}
library(scales)

h<-read.csv("house_sales.csv")
house_98105 <- h[h$ZipCode == 98105,]
lm_98105 <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms + Bedrooms + BldgGrade, data=house_98105)

df <- data.frame(
resid = residuals(lm_98105),
pred = predict(lm_98105))

theme_set(theme_light())


ggplot(df, aes(pred, abs(resid))) +
geom_point(color="red", size = .5) +
geom_smooth()+
labs(x="y", y="residuales")+
scale_x_continuous(labels = comma)
```

¿Qué podemos concluir sobre el supuesto  de homocedasticidad para este modelo?

2.- Considere el siguiente escenario en el que se  busca predecir el valor de las viviendas (variable *AdjSalePrice*) con base en diversos atributos tanto de las casas así como del vecindario en donde se ubican. 

La base de datos contiene 22,689 observaciones (viviendas vendidas entre 2014 y 2015) para un total de 22 variables con información para el Condado  King (que incluye al ciudad de Seattle, Washington, EUA). Para descripción completa de la base de datos Ver  [Data dictionary](https://www.kaggle.com/harlfoxem/housesalesprediction)
 
a) Estime el modelo de regresión lineal múltiple para la especificación: 

$$AdjSalePrice=\beta_0+\beta_1 SqFtTotLiving+\beta_2 SqFtLot+\beta_3 Bathrooms +\beta_4 Bedrooms +\beta_5 BldgGrade+\epsilon$$
Donde: 

AdjSalePrice: Precio de venta ajustado

SqFtTotLiving:Área de construcción en (pies cuadrados).

SqFtLot: Área del terreno (pies cuadrados).

Bathrooms: Número de Baños.

Bedrooms: Número recamaras. 

BldgGrade: calificación general dada a la vivienda con base en el sistema de clasificación del condado *King*.

Interprete el resultado para las variables que son estadisticamente significativas. En qué monto se incrementa el precio ante un aumento de Área de construcción de 1 sqrft adicional? y por 100?

b) ¿Qué proporción de la varianza es explicada por el modelo según el coeficiente de determinación? 


```{r, include=FALSE, warning=FALSE,message=FALSE}
h<-read.csv("house_sales.csv")

house_lm <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms + Bedrooms + BldgGrade, data = h, na.action = na.omit)

summary(house_lm)

 # vemos que por un sqrft adicional el precio se incrementa en $228 usd  y por 100 sqrft adicionales tenemos un incremento medio en el predio de $22,800. 

# El Coeficiente de determinación indica que el 54% de la variación en el precio de las viviendas es explicado por el modelo.
```


c) Aplique la metodología *Stepwise regression* y determine la especificación del modelo que minimiza el criterio de información  Akaike **AIC**. ¿Qué variables explicativas han sido excluidas del modelo?  ¿Cual es el valor que toma el **AIC** para el modelo inicial y el final? 

```{r,echo=FALSE, include=FALSE}

h<-read.csv("house_sales.csv")

house_full <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms +    Bedrooms + BldgGrade + PropertyType + NbrLivingUnits +  SqFtFinBasement + YrBuilt + YrRenovated + NewConstruction, data=h, na.action=na.omit)


step <- stepAIC(house_full, direction="both")
step

## AIC inicial 563193.9  final AIC=563188.5

## Variables excluidas: SqFtLot, NbrLivingUnits, YrRenovated y NewConstruction.
```


d)  Estime el modelo en la especificación siguiente: 

$$AdjSalePrice = \beta_0 +\beta_1 SqFtTotLiving + \beta_2 Bathrooms + \beta_3 Bedrooms + \beta_4 BldgGrade + \beta_5 PropertyType + \beta_6 SqFtFinBasement + \beta_7  YrBuilt, +\epsilon$$

Donde: 

SqFtFinBasement: Superficie sotano terminado (*finished basement*)

PropertyType: Tipo de propiedad. Variable categórica con tres factores: Multiplex, Single Family y Townhouse.

```{r,echo=FALSE}

h<-read.csv("house_sales.csv")
str(h)

lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms + Bedrooms + BldgGrade + YrBuilt+PropertyType+SqFtFinBasement, data=h)


```
e) ¿Cúal es el efecto al incrementar un dormitorio adicional sobre el precio promedio de las viviedas?

En la práctica  existe una relación positiva entre el valor y la superficie de la vivienda. En este caso tenemos un problema de regresores altamente correlacionados (Bedrooms, bathrooms y SqFtTotLiving), este efecto representa un problema y puede atenderse usando el enfoque para resolver **multicolinealidad** (remover variables que aportan información redundante).

Para ver con claridad el grado de correlación entre estas variables construimos una matriz de correlación para las variables del modelo. 

```{r,echo=FALSE, warning=FALSE,message=FALSE}
library(PerformanceAnalytics)
h2<-readRDS("h2.rds")
chart.Correlation(h2,histogram=TRUE, pch=19)


library(corrplot)
cor<-cor(h2)
corrplot.mixed(cor, lower.col = "black", number.cex = .7)
```

f) Estime nuevamente el modelo y excluya las variables redundantes (Bathrooms,SqFtTotLiving, SqFtFinBasement). ¿Cuál es el efecto ahora de la variable que registra el número de dormitorios?

```{r,echo=FALSE, eval=FALSE}
lm(formula = AdjSalePrice ~ Bedrooms + BldgGrade + PropertyType +
YrBuilt, data = h, na.action = na.omit)%>%
  summary()

## Un incremento de $27,132 en el precio promedio de la vivienda por dormitorio adicional. 
```



#### Omisión de Variables relevantes y efectos confusores.

Mientras con el tema de **multicolinealidad** el problema es la inclusión de variables redundantes, el escenario que causa el problema con las variables confusoras es la **omisión** de posibles variables relevantes. 

Consideremos el modelo sobre la relación entre el precio de las viviendas y sus características con datos a escala geográfica de Condado (King). 

En la siguiente especificación incluimos una variable denominada **ZipGroup**, es una variable categórica que clasifica  las viviendas acorde con el valor promedio de las viviendas del código postal en el que se ubican. 

Primero observemos los efectos principales sin la variable que captura la localización. 

Note que no se ha incluido una variable que explicitamente capture el efecto de la localización. 

La   especificación inicial a estimar es: 

$$AdjSalePrice=\beta_0+\beta_1 SqFtTotLiving+\beta_2 SqFtLot+\beta_3 Bathrooms +\beta_4 Bedrooms +\beta_5 BldgGrade+\epsilon$$

##### Ejemplo 3  

```{r, echo=FALSE}

### Para explicar variables  confusoras.

library(MASS)

h<-read.csv("house_sales.csv")

## Modelo regular sin confounder variable.

house_lm <- lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot + Bathrooms + Bedrooms + BldgGrade, data = h, na.action = na.omit)

summary(house_lm)

```


Note que de forma contra intuitiva la variable que captura la suerficie del terreno tiene un efecto negativo de igual forma la variable # baños, lo cual no es consistente en términos empíricos .

Estos efectos principales contraintuitivos  se asociacian probablemente con la omisión de variables explicativas relevantes (*confunding variables*). Ya que el modelo esta mezclando información de áreas muy heterogéneas.  

Ahora vamos a incluir la variable que captura la localización  **ZipGroup** (una posible variable confusora) y estimamos el modelo para determinar los efectos principales de variables para validar la relación teórica de causalidad.


```{r,eval=FALSE}

## DATA WRANGLING TO NCLUDE ZIPCODE VARIABLE
## Generar variable para grupo de zip code de barato a caro 1-5
zip_groups <- h %>%
  mutate(resid = residuals(house_lm)) %>%
  group_by(ZipCode) %>%
  summarize(med_resid = median(resid),
            cnt = n()) %>%
  # sort the zip codes by the median residual
  arrange(med_resid) %>%
  mutate(cum_cnt = cumsum(cnt),
         ZipGroup = factor(ntile(cum_cnt, 5)))
h <- h %>%
  left_join(select(zip_groups, ZipCode, ZipGroup), by='ZipCode')
```


```{r, echo=FALSE}
h_zipcode<-read.csv("houseszipcode.csv")%>%
  mutate(ZipGroup=as.factor(ZipGroup))

lm(AdjSalePrice ~ SqFtTotLiving + SqFtLot +
Bathrooms + Bedrooms +
BldgGrade + PropertyType + ZipGroup,
data=h_zipcode, na.action=na.omit)%>%
  summary()
```
Note ahora, que tanto la variable superficie como el número de *Bathrooms* resultan con un efecto principal positivo tal como  se espera. 

Note en segundo lugar que la inclusión de la variable que captura el efecto de localización es significativa estadísticamente y presenta mayor importancia para las viviendas ubicadas en códigos postales de mayor valor y por ejemplo aquellas propiedades ubicadas en el grupo 5 tiene en promedio un valor mayor en $\$340,000$  respecto a las viviendas en el  grupo 1.  

El efecto de la variable # domitorios es negativo ya que considerando una casa de la misma superficie contar con más dormitorios implica habitaciones de menor tamaño lo cual impacta el precio negativamente. 


### Términos de referencia.

**Homocedasticidad.** Los $\epsilon$ son iid (independientes e identicamente distribuidos) cada uno con media cero y varianza $\sigma^2$ (supuesto de homocedasticidad).  Note cuando este supuesto no se cumple tenemos un problema denominado: *heterocedasticidad**.

**Autocorrelación**

**Multicolinealidad**


