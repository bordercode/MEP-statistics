<!DOCTYPE html>
<html class="no-js" lang="en-us">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<title>PCA</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
	<meta name="generator" content="Hugo 0.54.0" />
	<meta property="og:title" content="PCA" />
<meta property="og:description" content="Análisis de Componentes Principales (PCA).MotivaciónEl método PCA permite reexpresar un conjunto multidimensional de datos (ej. k dimensiones) que contiene variables correlacionadas ( ej. que aportan información redundante), en un subconjnto de datos de menor dimensión.
Con PCA respondemos preguntas como:
¿Qué variables estan correlacionadas?
¿Podemos representar los datos de una manera más clara ej.mediante un menor número de dimensiones?
¿Qué variables tienen la mayor influencia para explicar la variación de los datos?" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/2019-02-24-pca/" />
<meta property="article:published_time" content="2019-01-28T21:14:14-05:00"/>
<meta property="article:modified_time" content="2019-01-28T21:14:14-05:00"/>

	
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">
	<link rel="stylesheet" href="/css/style.css">
	<link rel="shortcut icon" href="/favicon.ico">
		
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-124338825-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container">
		<div class="logo">
			<a class="logo__link" href="/" title="Estadística II" rel="home">
				<div class="logo__title">Estadística II</div>
				<div class="logo__tagline">Semestre II MEP 2019</div>
			</a>
		</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/about/">Maestría en estudios de población</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/post/syllabus/">Syllabus</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/post/2015-07-23-rl/">Regresion Lineal</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/post/2019-01-11-rl2/">RL-Simple</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/post/2019-01-15-rl3/">RL-Modelo</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/post/2019-01-25-rl4/">RL-Hipótesis</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/post/2019-01-25-rl5/">RL-Muiltiple</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/post/2019-02-05-rl6/">RL-Diagnóstico</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/post/2019-20-01-rl7-models/">RL-Multiple Cross-Section</a>
		</li>
		<li class="menu__item menu__item--active">
			<a class="menu__link" href="/post/2019-02-24-pca/">PCA</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/post/recursos-adicionales/">Recursos Adicionales</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/post/2018-12-30-ejercicios/">Ejercicios</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/post/2019-02-25-logistic/">Logistic R</a>
		</li>
	</ul>
</nav>

	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">PCA</h1>
			<div class="post__meta meta">
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 16 16"><path d="m8-.0000003c-4.4 0-8 3.6-8 8 0 4.4000003 3.6 8.0000003 8 8.0000003 4.4 0 8-3.6 8-8.0000003 0-4.4-3.6-8-8-8zm0 14.4000003c-3.52 0-6.4-2.88-6.4-6.4000003 0-3.52 2.88-6.4 6.4-6.4 3.52 0 6.4 2.88 6.4 6.4 0 3.5200003-2.88 6.4000003-6.4 6.4000003zm.4-10.4000003h-1.2v4.8l4.16 2.5600003.64-1.04-3.6-2.1600003z"/></svg>
	<time class="meta__text" datetime="2019-01-28T21:14:14">January 28, 2019</time>
</div>

<div class="meta__item-categories meta__item">
	<svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2l1 2h8v11h-16v-13z"/></svg>
	<span class="meta__text"><a class="meta__link" href="/categories/r" rel="category">R</a></span>
</div>
</div>
		</header><div class="content post__content clearfix">
			


<div id="analisis-de-componentes-principales-pca." class="section level2">
<h2>Análisis de Componentes Principales (PCA).</h2>
<div id="motivacion" class="section level4">
<h4>Motivación</h4>
<p>El método <strong>PCA</strong> permite reexpresar un conjunto multidimensional de datos (ej. k dimensiones) que contiene variables <strong>correlacionadas</strong> ( ej. que aportan información redundante), en un subconjnto de datos de menor dimensión.</p>
<p>Con <strong>PCA</strong> respondemos preguntas como:</p>
<p>¿Qué variables estan correlacionadas?</p>
<p>¿Podemos representar los datos de una manera más clara ej.mediante un menor número de dimensiones?</p>
<p>¿Qué variables tienen la mayor influencia para explicar la variación de los datos?</p>
<p>En este entorno cada variable la consideramos una dimensión.</p>
<p>Como ejemplo de la estructura de datos considere <span class="math inline">\(x_i\)</span> la observación del individuo <em>i</em> tomado de la variable <strong>k</strong> donde i varia de <strong>i</strong> a <strong>I</strong> y k a <strong>K</strong> Entonces en esta estructura la idea es reeprezar los datos a partir de un número de k´s. menor.</p>
<p>El <strong>PCA</strong> es una técnica para descubrir como variables <strong>numéricas</strong> <strong>“covarian”</strong>.</p>
<p>La aplicación del método reduce a un subconjunto de dimensiones (componentes principales) la información original. En análisis multivariado el concepto <em>Synthetic variable</em> denota la escencia del PCA.</p>
<p>Sugerencia: Jean Paul Bencecri sobre orígenes de este vínculo entre Multivariate Analysis DA y PCA.</p>
<p>La estructura de datos resultante maximiza la variación inherente al conjunto de datos.</p>
<p>Los componentes son combinaciones lineales de las variables originales, que controlan por el “ruido” (información redundante) en los datos. Nos interesa la <strong>señal</strong> no el <strong>ruido</strong>!</p>
<p>El <strong>componente principal</strong> (Eigenvector) es una combinación lineal de las variables consideradas para predicción. Este captura la mayor pate de la variación en las dimensiones consideradas.</p>
<div class="figure">
<img src="\img\pca1.jpg" />

</div>
<p>Note que el punto azul representa las coordenadas del valor promedio de las variables población y gasto en educación.</p>
<p>Note en este caso la linea central en el primer panel que captura la dirección por la que las dos variables se relacionan, en cierto sentido este vector sintetiza la realción entre las variables pobalción y gasto en educación para una muestra de 100 ciudades.</p>
<p>Si tomamos este vector y lo rotamos de tal manera que sea paralelo al eje horizontal ahora vemos una segunda fuente de variación (un segundo vector que captura otra dimensión de la variación en los datos) correspondiente a la variación vertical (representada por la distancia entre cada observación y nuestro primer vector).</p>
<p>En este ejemplo el plano formado por el vector en verde lo denominamos componente principal del conjunto de datos. Note que un segundo compoente principal se forma por el vector en azul.</p>
<div class="figure">
<img src="\img\pca2.jpg" />

</div>
<p>El método permite estimar la proporción de la variación que cada dimensión (componentes principales) captura, de tal forma que podemos identificar las variables de mayor relevancia para el análisis y conocer las variables correlacionadas.</p>
<p>Esa variación generalmente se representa mediante un <strong>Scree plot</strong> ( <strong>x</strong> axis componentes principales, <strong>y</strong> axis proporción de la varianza explicada).</p>
<p>En la estimación de <strong>PCA</strong>, el monto de la variación explicada que cada componente retiene se captura por el <strong>eigenvalor</strong> y se expresa como cociente de la variación total en los datos.</p>
<p>Uno de los uso de esta técnica es la construcción de modelos que permiten hacer clasificación como el modelo de regresión logistica (considerado parte del conjunto de GLM generalized linear models.).</p>
<p>El proceso de PCA aplicado a un problema de clasificación es el siguiente:</p>
<p>1.-Partimos de un conjunto de datos que contienen k dimensiones. La técnica de PCA permite reducir estas dimensiones al determinar los <strong>eigenvectores</strong> y los correspondientes eigenvalores (loadings), estos permiten determinar la proporción de la varianza explicada de cada componente con lo que se tiene un criterio para seleccionar aquellos que capturan la mayor varianza posible en el conjunto de los datos.</p>
<p>El número de eigenvectores a rentener es una cuestión empírica que depende del escenario de análisis, si bien un <em>Threshold</em> común es considerar aquellos pc que aportan <span class="math inline">\(\geq 85\%\)</span> de la varianza en los datos. de manera alternativa conservar aquellos Eigenvectores con Eigenvalores <span class="math inline">\(\lambda\geq1\)</span>.</p>
<p>2.- En el caso supervisado la nueva estrucura de datos de menor dimensión generada por el PCA puede ser utilizada para estimar modelos categóricos (de aprendizaje supervisado. ej tenemos una variable target para predecir) como regresión logística, <em>k nearest neighbors</em>, <em>linear discriminant Análysis</em> (<strong>LDA</strong>), etc,.</p>
<p>Estos permiten determinar la probabilidad de pertenencia a una categoría determinada dadas las característcas de cada observación.</p>
<p>El domino de aplicación es amplio. Una aproximación inicial a su uso es en la <strong>visualización</strong>, un proceso fundamental para comprender como se comportan nuestros datos antes de aplicar modelos de mayor complejidad. (VSM Vector Support Machine, NN Neural Networks , RF Random Forest,etc.,).</p>
<p>El dominio de aplicación de PCA incluye estudios poblacionales en areas como, migración,salud pública, indicadores socioeconómicos etc.</p>
</div>
<div id="elementos-teoricos." class="section level3">
<h3>Elementos teóricos.</h3>
<p>Antes de avanzar consideremos algunos fundamentos de algebra lineal necesarios para tener un mejor entendimiento de esta técnica:</p>
<p>Suponga que tiene la siguiente matriz con dimensiones mxn m=5, n=3</p>
<pre><code>##   Age Height  IQ
## 1  24    152 108
## 2  50    175 102
## 3  17    160  95
## 4  35    170  97
## 5  65    155  87</code></pre>
<p>Entonces podemos expresar el primer vector de esta matriz como <span class="math display">\[\vec{x_1}  = \begin{bmatrix} 24  \\152 \\ 108 \end{bmatrix}\]</span> Suponga que tenomos una matriz <em>A</em> nxn y un vector nx1 <strong>v</strong>. si multiplicamos <strong>v</strong> por <strong>A</strong> Obtenemos otro vector. La matriz <strong>A</strong>, ha realizado una transformación sobre el vector <strong>v</strong>.</p>
<div class="figure">
<img src="/img/matrix.jpg" />

</div>
<p>El producto resultante, la matriz transformada, cambió el tamaño, no la dirección del vector.</p>
<p><span class="math inline">\(\begin{bmatrix}2&amp;3\\1&amp;2\end{bmatrix} \begin{bmatrix}2\\5\end{bmatrix}=\begin{bmatrix}19\\12\end{bmatrix}\)</span></p>
<p>Recordemos la definición de Eigenvectores (vectores propios/característicos) y Eigenvalores. <strong>Eigenvectores</strong>. Un vector tal que cuando son transformados por el operador resultan en un múltiplo escalar de si mismos.</p>
<p>En otras palabras el vector característico de una transformación lineal es un vector <span class="math inline">\(\neq0\)</span> que cambia únicamente por un factor escalar cuando se le aplica la transformación lineal.</p>
<p>En este contexto el escalar <span class="math inline">\(\lambda\)</span> es denominado Eigenvalor (valor propio, o característico).</p>
<p><span class="math inline">\(A\vec{v.}=\lambda \vec{v.}\)</span></p>
<p>Donde <strong>v</strong> es un Eigenvector, <span class="math inline">\(\lambda\)</span> el Eigenvalor, A una matriz cuadrada.</p>
<p>Para mayor referencia considere la definición en el <a href="https://drive.google.com/file/d/1QIETnbXFvpk1GOOoFnA5l0SL7QDytN6h/view?usp=sharing">Anton H</a>:</p>
<div class="figure">
<img src="\img\pca3.jpg" />

</div>
<p>La aplicación de la teoría de vectores es particularmente útil para estudiar problemas de dinámica poblacional.</p>
<p><a href="https://medium.com/@andrew.chamberlain/using-eigenvectors-to-find-steady-state-population-flows-cd938f124764">Sugerencia leer Post</a></p>
<p><strong>Matriz simétrica:</strong> Una matriz <span class="math inline">\(mxn\)</span> se dice que es simétrica si <span class="math inline">\(A^T =A\)</span></p>
<p>Por ejemplo:</p>
<p><span class="math display">\[\begin{bmatrix}1&amp;2&amp;3\\2&amp;4&amp;5\\3&amp;5&amp;6\end{bmatrix}\]</span> En general la imagen de un vector <strong>x</strong> ante la multiplicación por una matriz <strong>A</strong> difiere tanto en dirección como en magnitud. Sin emabargo en el caso especial donde <strong>x</strong> es un eigenvector de <strong>A</strong>; la multiplicación por <strong>A</strong> deja la dirección sin cambio. Por ejemplo en <span class="math inline">\(\mathbb{R}^2\)</span> o <span class="math inline">\(\mathbb{R}^3\)</span> la multiplicación por <strong>A</strong> proyecta cada eigenvalor <strong>x</strong> de <strong>A</strong> (en caso de existir), a lo largo de la misma linea de <strong>x</strong> que atraviesa el origen.</p>
<p>Dependiendo del signo y magnitud del eigenvalor <span class="math inline">\(\lambda\)</span> correspondiente a <strong>x</strong>, la operacción <span class="math inline">\(Ax=\lambda x\)</span> comprime o expande <strong>x</strong> por un factor igual a <span class="math inline">\(\lambda\)</span>, con una dirección inversa en el caso de que <span class="math inline">\(\lambda&lt;0\)</span>.</p>
<p>La siguiente figura disponible en Anton H. ilustra este efecto.</p>
<div class="figure">
<img src="\img\pca4.jpg" />

</div>
<p>Note de la discusión anterior que la ecuación <span class="math inline">\(Ax=\lambda x\)</span> puede escribirse como <span class="math inline">\(Ax=\lambda I x\)</span> o bien de manera equivalente como:</p>
<p><span class="math display">\[(\lambda I -A)x=0\]</span></p>
<p>Para que <span class="math inline">\(\lambda\)</span> sea un eigenvalor de <strong>A</strong>, esta ecuación debe tener una solución <span class="math inline">\(\neq0\)</span> para <strong>x</strong>. Lo cual ocurre <strong>si y sólo si</strong> si la matriz de coeficientes <span class="math inline">\((\lambda I -A)\)</span> tiene un determinate <span class="math inline">\(=0\)</span>.</p>
<p>La observación anterior se expresa por la ecuación característica:</p>
<p><span class="math display">\[det(\lambda I -A)=0\]</span> En palabras. Si <strong>A</strong> es una matriz nxn, entonces <span class="math inline">\(\lambda\)</span> es un eigen vector de <strong>A</strong> <strong>si y solo si</strong>, este satisface la <strong>ecuación característica</strong>.</p>
<p>El proceso de PCA se basa en estos fundamentos teóricos para la determinación de los eigenvectores y correspondientes eigenvalores de la matriz de datos.</p>
<p>Entonces los eigenvectores capturan la “dirección” principal en la que se ubican los datos en el espacio vectorial y los eigenvalores actuan como ponderadores para determinar la extensión de estos eigenvectores.</p>
<p><strong>Vectores ortogonales:</strong> Dos vectores se consideran ortogonales cuando su producto punto:</p>
<p><span class="math display">\[\vec{u_.}\cdot \vec{v_.}=0\]</span></p>
<p><strong>Teorema:</strong></p>
<p>Sea <strong>A</strong> una matriz simétrica. Entonces, existen <strong>valores</strong> propios reales <span class="math inline">\(\lambda_1, \lambda_2,\lambda_3,...,\lambda_n,\)</span> y <strong>vectores</strong> propios ortogonales <span class="math inline">\(\vec{v_1},\vec{v_2},\vec{v_3},...\vec{v_n}\neq 0\)</span><br />
tal que <span class="math inline">\(A\vec{v_i}=\lambda \vec{v_i}\)</span> para <span class="math inline">\(i=1,2,...,n\)</span></p>
<p>Adicionalmente consideremos la siguientes observaciones:</p>
<ol style="list-style-type: decimal">
<li>Sea <span class="math inline">\(A \in \mathbb{R}\)</span> una matriz mXn. Entoces tanto <span class="math inline">\(A^T \cdot A\)</span> como <span class="math inline">\(A \cdot A^T\)</span> son simétricas.</li>
</ol>
<p><span class="math inline">\((AA^T)^T=(A^T)^T \cdot A^T=A \cdot A^T\)</span></p>
<p><span class="math inline">\((A^T A)^T=A^T \cdot (A^T)^T=A^T A\)</span></p>
<p>2.- Sea A una matriz mxn. Entonces la matriz <span class="math inline">\(AA^T\)</span> y <span class="math inline">\(A^TA\)</span> tienen los mismos valores propios <span class="math inline">\(\neq0\)</span></p>
<p>Considere un vector propio <span class="math inline">\(v\)</span> <span class="math inline">\(\neq0\)</span> de <span class="math inline">\(A^TA\)</span> con valor propio <span class="math inline">\(\lambda \neq0\)</span> ej. <span class="math inline">\((A^TA)v=\lambda v\)</span></p>
<p>Ahora si multiplicamos la matriz anterior por A tenemos <span class="math display">\[(AA^T)(Av)=\lambda(Av)\]</span> Lo que implica que <span class="math inline">\(Av\)</span> es un vector propio de <span class="math inline">\(A A^T\)</span> con Eigenvalor <span class="math inline">\(\lambda\)</span>. Entonces los valores característicos de <span class="math inline">\(A^TA\)</span> son los mismos que <span class="math inline">\(AA^T\)</span>.</p>
<p>El uso de esta propuesta es importante por que permite simplificar el computo para estimar los eigenvalores por ejemplo considere una matriz de 400x3 entonces <span class="math inline">\(A^TA\)</span> es una matriz de 400X400 con 400 <span class="math inline">\(\lambda\)</span>’s pero <span class="math inline">\(A^TA\)</span> tiene solo 3 <span class="math inline">\(\lambda\)</span>’s,</p>
<p><strong>Para enfatizar:</strong></p>
<p>Con la técnica de PCA determinamos los vectores propios, estos vectores capturan el espacio común a lo largo del cuál se registra la variación de los datos y los valores propios, que dan cuenta de la magnitud de la varianza que cada vector propio explica.</p>
<p>Para comprender el potencial del uso de PCA para el análisis de datos primero ubiquemos el proceso que sigue esta técnica en el esquema más amplio del análisis y en seguida haremos una serie de ejemplos comenzando con una aplicación para una matriz de 500+ , 30.</p>
</div>
</div>
<div id="el-proceso-de-calculo." class="section level2">
<h2>El proceso de cálculo.</h2>
<p>Primero obtenemos la media de las <strong>m</strong> variables en nuestra matriz. Y almacenamos estos valores en un vector en el espacio <span class="math inline">\(\mathbb{R}^m\)</span></p>
<p><span class="math inline">\(\vec{\mu}=\frac{1}{n-1}(\vec{x}_1+\vec{x}_2+\ldots+\vec{x}_n)\)</span></p>
<p>En segundo lugar estandarizamos los datos (media cero, desviación estándar=1), este paso permite homogeneizar las escalas para evitar que las diferencias en las unidaes de medida generen distorciones en el cálculo de los componentes principales. Las siguiente gráfica muestra el efecto de la estandarización (Scale).</p>
<div class="figure">
<img src="/img/v.jpg" />

</div>
<p>El proceso de estandarización permite obtener una matriz escalada:</p>
<p><span class="math inline">\(B=\begin{bmatrix}\vec{x}_1-\vec{\mu}|\ldots|\vec{x}_n-\vec{\mu}\end{bmatrix}\)</span></p>
<p>Ahora estimamos la matriz de covarianza <strong>S</strong> que se define como <span class="math display">\[\displaystyle S=\frac{1}{n-1}BB^T\]</span></p>
<p>De la discusión previa podemos ver que S es una matriz simétrica mxm.</p>
<p>Consideremos como ejemplo: <span class="math display">\[\vec{x}_1=\begin{bmatrix}a_1\\a_2\\a_3\end{bmatrix}, \vec{x}_2=\begin{bmatrix}b_1\\b_2\\b_3\end{bmatrix}, \vec{x}_3=\begin{bmatrix}c_1\\c_2\\c_3\end{bmatrix}, \vec{\mu}=\begin{bmatrix}\mu_1\\\mu_2\\\mu_3\end{bmatrix}\]</span> Con matriz escalada <strong>B</strong>: <span class="math display">\[B=\begin{bmatrix}a_1-\mu_1&amp;b_1-\mu_1&amp;c_1-\mu_1\\a_2-\mu_2&amp;b_2-\mu_2&amp;c_2-\mu_2\\a_3-\mu_3&amp;b_3-\mu_3&amp;c_3-\mu_3\end{bmatrix}\]</span> Ahora veamos los elementos de la matriz de covarianza <strong>S</strong></p>
<p><span class="math display">\[\displaystyle S_{11}=\frac{1}{n-1}((a_1-\mu_1)^2+(b_1-\mu_1)^2+(c_1-\mu_1)^2)=\mbox{ Varianza de la primera variable }\]</span> De manera similar los elementos <span class="math inline">\(S_{22}, S_{33}\)</span> representan la varianza de la segunda y tercera variables respectivamente.</p>
<p>En segiuida tenemos la covarinza entre la primera y segunda variables:</p>
<p><span class="math display">\[\displaystyle S_{12}=\frac{1}{n-1}((a_1-\mu_1)(a_2-\mu_2)+(b_1-\mu_1)(b_2-\mu_2)+(c_1-\mu_1)(c_2-\mu_2))=S_{21}\]</span> Y generalizando tenemos que:</p>
<ul>
<li><p><span class="math inline">\(S_{ii}\)</span> es la <em>varianza</em> de la <span class="math inline">\(i_{esima}\)</span> variable.</p></li>
<li><p><span class="math inline">\(S_{ij}\)</span> con <span class="math inline">\(i\neq j\)</span> es la covarianza entre las variables i, j.</p></li>
</ul>
<p>El siguiente paso es aplicar el teorema descrito en la sección previa, dado que la matriz <strong>S</strong> es simétrica tendrá Eigenvalores <span class="math inline">\(\lambda_1\geq \lambda_2\geq \ldots\geq\lambda_m\geq 0\)</span> y Eigenvectores ortogonales <span class="math inline">\(\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_m\)</span> Estos vectores son denominados los componentes principales de la base de datos.</p>
<p>Note que el segundo componente principal es una combinación lineal de las variables no correlaciondas con pc1, condición que se deduce de la definición de vectores ortogonales.</p>
<p>Paso siguiente, determiar la varianza total <strong>T</strong> en los datos, que es igual a la <strong>traza</strong><a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> de la matriz de covarianza**. Así mismo la traza de la matriz es igual a la suma de sus Eigenvalores: <span class="math display">\[T=\lambda_1+\lambda_2+\ldots+\lambda_m\]</span> ¿Cómo interpretamos los resultados del PCA?</p>
<p>La dirección dada por el vector <span class="math inline">\(\vec{v_{1}}\)</span> <span class="math inline">\(\mathbb{R}^m\)</span> denominado el <strong>primer componente principal</strong> da cuenta de un monto igual a <span class="math inline">\(\lambda_1\)</span> de la varianza total T.</p>
<p>Es una fracción <span class="math inline">\(\displaystyle\frac{\lambda_1}{T}\)</span> de la varianza total. De igual forma, la segunda dirección principal <span class="math inline">\(\vec{v}_2\)</span>, que da cuenta de una fracción <span class="math inline">\(\displaystyle\frac{\lambda_2}{T}\)</span>.</p>
<ul>
<li><p>El vector <span class="math inline">\(\vec{v}_1\in \mathbb{R}^m\)</span> constituye la dirección de mayor importancia en el conjunto de datos.</p></li>
<li><p>Entre las direcciones ortogonales a <span class="math inline">\(\vec{v}_1\)</span>, <span class="math inline">\(\vec{v}_2\)</span> es la dirección más significativa.</p></li>
<li><p>De forma similar entre las direcciones ortogonales a <span class="math inline">\(\vec{v}_1\)</span> y <span class="math inline">\(\vec{v}_2\)</span>, <span class="math inline">\(\vec{v}_3\)</span> es la dirección más significativa y así sucesivamentente.</p></li>
</ul>
<p>¿Que conjunto de datos tenemos como resultado del PCA?</p>
<p>El procedimiento de PCA ha generado una nueva estructura de los datos (reducción en las dimensiones) por lo que ahora podemos representar digamos un espacio vectorial que era digamos <span class="math inline">\(\mathbb{R}^{30}\)</span> mediante una estructura de compoentes principales de menor dimensión con las observaciones del conjunto de datos conformando clusters definidos en función de los Eigenvectores.</p>
</div>
<div id="ejemplo-1-base-de-datos-sobre-cancer-de-mama." class="section level2">
<h2>Ejemplo 1 Base de datos sobre cáncer de mama.</h2>
<p>El propósito es determinar un subconjunto de variables que capturen la mayor cantidad de información posible (ej. que capturen la mayor variación posible).</p>
<p>Esta técnica permite además visualizar los vectores a lo largo de los cuales la mayor variación posible es capturada (2D).</p>
<p>Este subconjunto de variables consituyen una síntensis que representa los eigenvectores de la matriz original.</p>
<p><a href="http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29">Background</a></p>
<pre><code>## corrplot 0.84 loaded</code></pre>
<pre><code>## &#39;data.frame&#39;:    569 obs. of  33 variables:
##  $ id                     : int  842302 842517 84300903 84348301 84358402 843786 844359 84458202 844981 84501001 ...
##  $ diagnosis              : Factor w/ 2 levels &quot;B&quot;,&quot;M&quot;: 2 2 2 2 2 2 2 2 2 2 ...
##  $ radius_mean            : num  18 20.6 19.7 11.4 20.3 ...
##  $ texture_mean           : num  10.4 17.8 21.2 20.4 14.3 ...
##  $ perimeter_mean         : num  122.8 132.9 130 77.6 135.1 ...
##  $ area_mean              : num  1001 1326 1203 386 1297 ...
##  $ smoothness_mean        : num  0.1184 0.0847 0.1096 0.1425 0.1003 ...
##  $ compactness_mean       : num  0.2776 0.0786 0.1599 0.2839 0.1328 ...
##  $ concavity_mean         : num  0.3001 0.0869 0.1974 0.2414 0.198 ...
##  $ concave.points_mean    : num  0.1471 0.0702 0.1279 0.1052 0.1043 ...
##  $ symmetry_mean          : num  0.242 0.181 0.207 0.26 0.181 ...
##  $ fractal_dimension_mean : num  0.0787 0.0567 0.06 0.0974 0.0588 ...
##  $ radius_se              : num  1.095 0.543 0.746 0.496 0.757 ...
##  $ texture_se             : num  0.905 0.734 0.787 1.156 0.781 ...
##  $ perimeter_se           : num  8.59 3.4 4.58 3.44 5.44 ...
##  $ area_se                : num  153.4 74.1 94 27.2 94.4 ...
##  $ smoothness_se          : num  0.0064 0.00522 0.00615 0.00911 0.01149 ...
##  $ compactness_se         : num  0.049 0.0131 0.0401 0.0746 0.0246 ...
##  $ concavity_se           : num  0.0537 0.0186 0.0383 0.0566 0.0569 ...
##  $ concave.points_se      : num  0.0159 0.0134 0.0206 0.0187 0.0188 ...
##  $ symmetry_se            : num  0.03 0.0139 0.0225 0.0596 0.0176 ...
##  $ fractal_dimension_se   : num  0.00619 0.00353 0.00457 0.00921 0.00511 ...
##  $ radius_worst           : num  25.4 25 23.6 14.9 22.5 ...
##  $ texture_worst          : num  17.3 23.4 25.5 26.5 16.7 ...
##  $ perimeter_worst        : num  184.6 158.8 152.5 98.9 152.2 ...
##  $ area_worst             : num  2019 1956 1709 568 1575 ...
##  $ smoothness_worst       : num  0.162 0.124 0.144 0.21 0.137 ...
##  $ compactness_worst      : num  0.666 0.187 0.424 0.866 0.205 ...
##  $ concavity_worst        : num  0.712 0.242 0.45 0.687 0.4 ...
##  $ concave.points_worst   : num  0.265 0.186 0.243 0.258 0.163 ...
##  $ symmetry_worst         : num  0.46 0.275 0.361 0.664 0.236 ...
##  $ fractal_dimension_worst: num  0.1189 0.089 0.0876 0.173 0.0768 ...
##  $ X                      : logi  NA NA NA NA NA NA ...</code></pre>
<pre><code>##  [1] &quot;id&quot;                      &quot;diagnosis&quot;              
##  [3] &quot;radius_mean&quot;             &quot;texture_mean&quot;           
##  [5] &quot;perimeter_mean&quot;          &quot;area_mean&quot;              
##  [7] &quot;smoothness_mean&quot;         &quot;compactness_mean&quot;       
##  [9] &quot;concavity_mean&quot;          &quot;concave.points_mean&quot;    
## [11] &quot;symmetry_mean&quot;           &quot;fractal_dimension_mean&quot; 
## [13] &quot;radius_se&quot;               &quot;texture_se&quot;             
## [15] &quot;perimeter_se&quot;            &quot;area_se&quot;                
## [17] &quot;smoothness_se&quot;           &quot;compactness_se&quot;         
## [19] &quot;concavity_se&quot;            &quot;concave.points_se&quot;      
## [21] &quot;symmetry_se&quot;             &quot;fractal_dimension_se&quot;   
## [23] &quot;radius_worst&quot;            &quot;texture_worst&quot;          
## [25] &quot;perimeter_worst&quot;         &quot;area_worst&quot;             
## [27] &quot;smoothness_worst&quot;        &quot;compactness_worst&quot;      
## [29] &quot;concavity_worst&quot;         &quot;concave.points_worst&quot;   
## [31] &quot;symmetry_worst&quot;          &quot;fractal_dimension_worst&quot;
## [33] &quot;X&quot;</code></pre>
<p>Observamos que este <strong>df</strong> contine 569 obsrevaciones, en este caso pacientes con diagnóstico de cáncer de mama. Tenemos 30 variables. el df constituye una matriz nxm: 569x30.</p>
<p>Sin embargo, es posible que en las 30 variables tenemos cierta correlación entre ellas. La técnica de <strong>PCA</strong> es útil en este caso por que podemos representar las 30 dimensiones mediante un subconjunto menor que capture la mayor variación posible contenida el la matriz. Esta reducción se basa en la trayectoria de los eigenvectores de la matriz original.</p>
<p>Entre las variables tenemos una denominada diagnosis, esta indica el tipo de tumor benigno <strong>B</strong> o maligno <strong>M</strong>. esta variable es considerada el target para la clasificación posterior, por ejemplo mediante un modelo de regresión logística.</p>
<div id="fase-eda." class="section level4">
<h4>1. Fase EDA.</h4>
<pre class="r"><code># descriptive stats para todas las variables:
summary(bc)</code></pre>
<pre><code>##        id            diagnosis  radius_mean      texture_mean  
##  Min.   :     8670   B:357     Min.   : 6.981   Min.   : 9.71  
##  1st Qu.:   869218   M:212     1st Qu.:11.700   1st Qu.:16.17  
##  Median :   906024             Median :13.370   Median :18.84  
##  Mean   : 30371831             Mean   :14.127   Mean   :19.29  
##  3rd Qu.:  8813129             3rd Qu.:15.780   3rd Qu.:21.80  
##  Max.   :911320502             Max.   :28.110   Max.   :39.28  
##  perimeter_mean     area_mean      smoothness_mean   compactness_mean 
##  Min.   : 43.79   Min.   : 143.5   Min.   :0.05263   Min.   :0.01938  
##  1st Qu.: 75.17   1st Qu.: 420.3   1st Qu.:0.08637   1st Qu.:0.06492  
##  Median : 86.24   Median : 551.1   Median :0.09587   Median :0.09263  
##  Mean   : 91.97   Mean   : 654.9   Mean   :0.09636   Mean   :0.10434  
##  3rd Qu.:104.10   3rd Qu.: 782.7   3rd Qu.:0.10530   3rd Qu.:0.13040  
##  Max.   :188.50   Max.   :2501.0   Max.   :0.16340   Max.   :0.34540  
##  concavity_mean    concave.points_mean symmetry_mean   
##  Min.   :0.00000   Min.   :0.00000     Min.   :0.1060  
##  1st Qu.:0.02956   1st Qu.:0.02031     1st Qu.:0.1619  
##  Median :0.06154   Median :0.03350     Median :0.1792  
##  Mean   :0.08880   Mean   :0.04892     Mean   :0.1812  
##  3rd Qu.:0.13070   3rd Qu.:0.07400     3rd Qu.:0.1957  
##  Max.   :0.42680   Max.   :0.20120     Max.   :0.3040  
##  fractal_dimension_mean   radius_se        texture_se      perimeter_se   
##  Min.   :0.04996        Min.   :0.1115   Min.   :0.3602   Min.   : 0.757  
##  1st Qu.:0.05770        1st Qu.:0.2324   1st Qu.:0.8339   1st Qu.: 1.606  
##  Median :0.06154        Median :0.3242   Median :1.1080   Median : 2.287  
##  Mean   :0.06280        Mean   :0.4052   Mean   :1.2169   Mean   : 2.866  
##  3rd Qu.:0.06612        3rd Qu.:0.4789   3rd Qu.:1.4740   3rd Qu.: 3.357  
##  Max.   :0.09744        Max.   :2.8730   Max.   :4.8850   Max.   :21.980  
##     area_se        smoothness_se      compactness_se      concavity_se    
##  Min.   :  6.802   Min.   :0.001713   Min.   :0.002252   Min.   :0.00000  
##  1st Qu.: 17.850   1st Qu.:0.005169   1st Qu.:0.013080   1st Qu.:0.01509  
##  Median : 24.530   Median :0.006380   Median :0.020450   Median :0.02589  
##  Mean   : 40.337   Mean   :0.007041   Mean   :0.025478   Mean   :0.03189  
##  3rd Qu.: 45.190   3rd Qu.:0.008146   3rd Qu.:0.032450   3rd Qu.:0.04205  
##  Max.   :542.200   Max.   :0.031130   Max.   :0.135400   Max.   :0.39600  
##  concave.points_se   symmetry_se       fractal_dimension_se
##  Min.   :0.000000   Min.   :0.007882   Min.   :0.0008948   
##  1st Qu.:0.007638   1st Qu.:0.015160   1st Qu.:0.0022480   
##  Median :0.010930   Median :0.018730   Median :0.0031870   
##  Mean   :0.011796   Mean   :0.020542   Mean   :0.0037949   
##  3rd Qu.:0.014710   3rd Qu.:0.023480   3rd Qu.:0.0045580   
##  Max.   :0.052790   Max.   :0.078950   Max.   :0.0298400   
##   radius_worst   texture_worst   perimeter_worst    area_worst    
##  Min.   : 7.93   Min.   :12.02   Min.   : 50.41   Min.   : 185.2  
##  1st Qu.:13.01   1st Qu.:21.08   1st Qu.: 84.11   1st Qu.: 515.3  
##  Median :14.97   Median :25.41   Median : 97.66   Median : 686.5  
##  Mean   :16.27   Mean   :25.68   Mean   :107.26   Mean   : 880.6  
##  3rd Qu.:18.79   3rd Qu.:29.72   3rd Qu.:125.40   3rd Qu.:1084.0  
##  Max.   :36.04   Max.   :49.54   Max.   :251.20   Max.   :4254.0  
##  smoothness_worst  compactness_worst concavity_worst  concave.points_worst
##  Min.   :0.07117   Min.   :0.02729   Min.   :0.0000   Min.   :0.00000     
##  1st Qu.:0.11660   1st Qu.:0.14720   1st Qu.:0.1145   1st Qu.:0.06493     
##  Median :0.13130   Median :0.21190   Median :0.2267   Median :0.09993     
##  Mean   :0.13237   Mean   :0.25427   Mean   :0.2722   Mean   :0.11461     
##  3rd Qu.:0.14600   3rd Qu.:0.33910   3rd Qu.:0.3829   3rd Qu.:0.16140     
##  Max.   :0.22260   Max.   :1.05800   Max.   :1.2520   Max.   :0.29100     
##  symmetry_worst   fractal_dimension_worst    X          
##  Min.   :0.1565   Min.   :0.05504         Mode:logical  
##  1st Qu.:0.2504   1st Qu.:0.07146         NA&#39;s:569      
##  Median :0.2822   Median :0.08004                       
##  Mean   :0.2901   Mean   :0.08395                       
##  3rd Qu.:0.3179   3rd Qu.:0.09208                       
##  Max.   :0.6638   Max.   :0.20750</code></pre>
<pre class="r"><code>diagnosis &lt;- as.numeric(bc$diagnosis == &quot;M&quot;)
# Cuantos casos por tipo?

count_diag&lt;-group_by(bc,diagnosis)%&gt;%
  summarise(total=n())

# Valor promedio de todas las variables  para cada grupo?
bc_mean&lt;-group_by(bc,diagnosis)%&gt;%
  summarise_all(funs(mean))</code></pre>
<pre><code>## Warning: funs() is soft deprecated as of dplyr 0.8.0
## please use list() instead
## 
## # Before:
## funs(name = f(.)
## 
## # After: 
## list(name = ~f(.))
## This warning is displayed once per session.</code></pre>
<pre class="r"><code># Valor promedio de todas las variables  para cada grupo y tdo el df?
bc_std&lt;-group_by(bc,diagnosis)%&gt;%
  summarise_all(funs(sd))

bc_std.all&lt;-(bc)%&gt;%
  summarise_all((funs(sd)))</code></pre>
<pre><code>## Warning in var(if (is.vector(x) || is.factor(x)) x else as.double(x), na.rm = na.rm): Calling var(x) on a factor x is deprecated and will become an error.
##   Use something like &#39;all(duplicated(x)[-1L])&#39; to test for a constant vector.</code></pre>
<pre class="r"><code># Matriz de correlación
bc_matrix&lt;-select(bc, -c(diagnosis,id, X))

# Rename cols para mejor interpretación.

cNames &lt;- c(&quot;rad_m&quot;,&quot;txt_m&quot;,&quot;per_m&quot;,
                 &quot;are_m&quot;,&quot;smt_m&quot;,&quot;cmp_m&quot;,&quot;con_m&quot;,
                 &quot;ccp_m&quot;,&quot;sym_m&quot;,&quot;frd_m&quot;,
                 &quot;rad_se&quot;,&quot;txt_se&quot;,&quot;per_se&quot;,&quot;are_se&quot;,&quot;smt_se&quot;,
                 &quot;cmp_se&quot;,&quot;con_se&quot;,&quot;ccp_se&quot;,&quot;sym_se&quot;,
                 &quot;frd_se&quot;,&quot;rad_w&quot;,&quot;txt_w&quot;,&quot;per_w&quot;,
                 &quot;are_w&quot;,&quot;smt_w&quot;,&quot;cmp_w&quot;,&quot;con_w&quot;,
                 &quot;ccp_w&quot;,&quot;sym_w&quot;,&quot;frd_w&quot;)


colnames(bc_matrix) &lt;- cNames

M &lt;- round(cor(bc_matrix), 2)
corrplot(M, diag = FALSE, method=&quot;color&quot;, order=&quot;FPC&quot;, tl.srt = 90)</code></pre>
<p><img src="/post/2019-02-24-PCA_files/figure-html/unnamed-chunk-4-1.png" width="672" /> Note el grado de correlación entre las variables de la muestra. Soporte en favor de la decisión de usar la metodología de PCA.</p>
</div>
<div id="implementacion.-pre-proceso-scaling" class="section level3">
<h3>2. Implementación. Pre proceso (Scaling)</h3>
<p>Una pregunta escencial <strong>previo</strong> a la implementación de <strong>PCA</strong> es si debemos estandarizar (scalar) nuestra matriz. ( variables con media=0, std dev=1).( Z-Score Normalization. <em>Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one.</em>)</p>
<p>Nota sobre el proceso de estandarización.</p>
<p><em>We can think of Principle Component Analysis (PCA) as being a prime example of when normalization is important. In PCA we are interested in the components that maximize the variance. If one component (e.g. human height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds with the ‘weight’ axis, if those features are not scaled. As a change in height of one meter can be considered much more important than the change in weight of one kilogram, this is clearly incorrect.</em></p>
<p>La respuesta es directa, cuando las unidades de medida nos son homogéneas es <strong>necesario</strong> escalar los datos.</p>
<p>Escalar los datos implica utilizar la <strong>matriz de correlación</strong>. Utilizar los datos sin escalar require utilizar la correspondiente <strong>matriz de covarianza</strong>.</p>
<p>De lo contrario los <strong>eigenvectores</strong> resultantes en el proceso de estimación de <strong>PCA</strong> no aportan información consistente.(ej. la importancia de cada variable y sus unidades de medida tendrá un impacto en la organización de los <strong>eigenvectores</strong> resultantes distorsionando la varianza real explicada por cada componente).</p>
<div id="estimacion-de-pca-usando-la-matriz-de-correlacion." class="section level4">
<h4>3. Estimacion de PCA usando la matriz de correlación.</h4>
<pre><code>## Importance of components:
##                           PC1    PC2     PC3     PC4     PC5     PC6
## Standard deviation     3.6444 2.3857 1.67867 1.40735 1.28403 1.09880
## Proportion of Variance 0.4427 0.1897 0.09393 0.06602 0.05496 0.04025
## Cumulative Proportion  0.4427 0.6324 0.72636 0.79239 0.84734 0.88759
##                            PC7     PC8    PC9    PC10   PC11    PC12
## Standard deviation     0.82172 0.69037 0.6457 0.59219 0.5421 0.51104
## Proportion of Variance 0.02251 0.01589 0.0139 0.01169 0.0098 0.00871
## Cumulative Proportion  0.91010 0.92598 0.9399 0.95157 0.9614 0.97007
##                           PC13    PC14    PC15    PC16    PC17    PC18
## Standard deviation     0.49128 0.39624 0.30681 0.28260 0.24372 0.22939
## Proportion of Variance 0.00805 0.00523 0.00314 0.00266 0.00198 0.00175
## Cumulative Proportion  0.97812 0.98335 0.98649 0.98915 0.99113 0.99288
##                           PC19    PC20   PC21    PC22    PC23   PC24
## Standard deviation     0.22244 0.17652 0.1731 0.16565 0.15602 0.1344
## Proportion of Variance 0.00165 0.00104 0.0010 0.00091 0.00081 0.0006
## Cumulative Proportion  0.99453 0.99557 0.9966 0.99749 0.99830 0.9989
##                           PC25    PC26    PC27    PC28    PC29    PC30
## Standard deviation     0.12442 0.09043 0.08307 0.03987 0.02736 0.01153
## Proportion of Variance 0.00052 0.00027 0.00023 0.00005 0.00002 0.00000
## Cumulative Proportion  0.99942 0.99969 0.99992 0.99997 1.00000 1.00000</code></pre>
<pre class="r"><code># Set up 1 x 2 plotting grid
par(mfrow = c(1, 2))

# Calculate variability of each component
pr.var &lt;- bc.pr$sdev ^ 2

# Variance explained by each principal component: pve
pve &lt;- pr.var/sum(pr.var)

# Eigen values  (loadings)
round(pr.var, 2)</code></pre>
<pre><code>##  [1] 13.28  5.69  2.82  1.98  1.65  1.21  0.68  0.48  0.42  0.35  0.29
## [12]  0.26  0.24  0.16  0.09  0.08  0.06  0.05  0.05  0.03  0.03  0.03
## [23]  0.02  0.02  0.02  0.01  0.01  0.00  0.00  0.00</code></pre>
<pre class="r"><code>pr.var</code></pre>
<pre><code>##  [1] 1.328161e+01 5.691355e+00 2.817949e+00 1.980640e+00 1.648731e+00
##  [6] 1.207357e+00 6.752201e-01 4.766171e-01 4.168948e-01 3.506935e-01
## [11] 2.939157e-01 2.611614e-01 2.413575e-01 1.570097e-01 9.413497e-02
## [16] 7.986280e-02 5.939904e-02 5.261878e-02 4.947759e-02 3.115940e-02
## [21] 2.997289e-02 2.743940e-02 2.434084e-02 1.805501e-02 1.548127e-02
## [26] 8.177640e-03 6.900464e-03 1.589338e-03 7.488031e-04 1.330448e-04</code></pre>
<pre class="r"><code># Percent variance explained
round(pve, 2)</code></pre>
<pre><code>##  [1] 0.44 0.19 0.09 0.07 0.05 0.04 0.02 0.02 0.01 0.01 0.01 0.01 0.01 0.01
## [15] 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00
## [29] 0.00 0.00</code></pre>
<pre class="r"><code># Cummulative percent explained
round(cumsum(pve), 2)</code></pre>
<pre><code>##  [1] 0.44 0.63 0.73 0.79 0.85 0.89 0.91 0.93 0.94 0.95 0.96 0.97 0.98 0.98
## [15] 0.99 0.99 0.99 0.99 0.99 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00
## [29] 1.00 1.00</code></pre>
<p>El 89% de la varianza es explicada por los primeros 6 componentes. Adicionalmente vemos que eigenvalores&gt;1 para los 6 pc que explican el 89% de la varianza. Este será el criterio de selección para retenter en este caso los pc (ej . si los eigenvalores&gt;1).</p>
<p>Scree plot para mostrar la varianza explicada por cada componente.</p>
<p><img src="/post/2019-02-24-PCA_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<pre class="r"><code># Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = &quot;Principal Component&quot;, 
     ylab = &quot;Cumulative Proportion of Variance Explained&quot;, 
     ylim = c(0, 1), type = &quot;b&quot;)</code></pre>
<p><img src="/post/2019-02-24-PCA_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="visualizacion-de-los-datos.-plot-de-observaciones-para-compoenetes-1-y-2." class="section level4">
<h4>Visualización de los datos. Plot de Observaciones para compoenetes 1 y 2.</h4>
<pre class="r"><code>plot(bc.pr$x[, c(1, 2)], col = (diagnosis + 1), 
     xlab = &quot;PC1&quot;, ylab = &quot;PC2&quot;)
legend(x=&quot;topleft&quot;, pch=1, col = c(&quot;red&quot;, &quot;black&quot;), legend = c(&quot;B&quot;, &quot;M&quot;))</code></pre>
<p><img src="/post/2019-02-24-PCA_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
</div>
<div id="aproximacion-a-la-solucion-del-problema-como-un-escenario-de-supervised-learning." class="section level4">
<h4>Aproximación a la solución del problema como un escenario de <em>supervised learning</em>.</h4>
<p>La variable que nos interesa predecir es <em>diagnosis</em>, ej. determinar si el tumor es maligno o benigno (variable dependiente) con base en las 30 variables independientes.</p>
<p>Primero aplicamos el PCA para reducir las dimensiones del df. esto nos permite clasificar a los pacientes con base en los atributos y visualizar el resultado mendiante un menor número de dimensiones. Get jupyter notebook para mayor referencia sobre el caso de estudio en:</p>
<p><a href="https://www.kaggle.com/shravank/predicting-breast-cancer-using-pca-lda-in-r" class="uri">https://www.kaggle.com/shravank/predicting-breast-cancer-using-pca-lda-in-r</a></p>
</div>
</div>
</div>
<div id="ejemplo-2-iris." class="section level1">
<h1>Ejemplo 2 Iris.</h1>
<p>Ver Jupyter notebook</p>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Traza: Suma de los elementos de la diagonal de una matriz nxn<a href="#fnref1">↩</a></p></li>
</ol>
</div>

		</div>
		
<div class="post__tags tags clearfix">
	<svg class="icon icon-tag" width="16" height="16" viewBox="0 0 16 16"><path d="M16 9.5c0 .373-.24.74-.5 1l-5 5c-.275.26-.634.5-1 .5-.373 0-.74-.24-1-.5L1 8a2.853 2.853 0 0 1-.7-1C.113 6.55 0 5.973 0 5.6V1.4C0 1.034.134.669.401.401.67.134 1.034 0 1.4 0h4.2c.373 0 .95.113 1.4.3.45.187.732.432 1 .7l7.5 7.502c.26.274.5.632.5.998zM3.5 5a1.5 1.5 0 1 0 0-3 1.5 1.5 0 0 0 0 3z"/></svg>
	<ul class="tags__list">
		<li class="tags__item"><a class="tags__link btn" href="/tags/pca/" rel="tag">PCA</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/clustering/" rel="tag">clustering</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/classification/" rel="tag">classification</a></li>
		<li class="tags__item"><a class="tags__link btn" href="/tags/unsupervised-statistical-learning/" rel="tag">unsupervised statistical learning</a></li>
	</ul>
</div>
	</article>
</main>

<div class="authorbox clearfix">
	<figure class="authorbox__avatar">
		<img alt="José Luis Manzanares Rivera avatar" src="/img/aavatar.jpg" class="avatar" height="90" width="90">
	</figure>
	<div class="authorbox__header">
		<span class="authorbox__name">About José Luis Manzanares Rivera</span>
	</div>
	<div class="authorbox__description">
		José Luis is a data scientist. He is currently conducting applied research at El Colegio de la Frontera Norte.
	</div>
</div>

<nav class="post-nav flex">
	<div class="post-nav__item post-nav__item--prev">
		<a class="post-nav__link" href="/post/2019-01-25-rl5/" rel="prev"><span class="post-nav__caption">«&thinsp;Previous</span><p class="post-nav__post-title">RL-Multiple</p></a>
	</div>
	<div class="post-nav__item post-nav__item--next">
		<a class="post-nav__link" href="/post/2019-02-05-rl6/" rel="next"><span class="post-nav__caption">Next&thinsp;»</span><p class="post-nav__post-title">RL-Diagnóstico</p></a>
	</div>
</nav>

<section class="comments">
	<div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "4insight" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


			</div>
			<aside class="sidebar"><div class="widget-search widget">
	<form class="widget-search__form" role="search" method="get" action="https://google.com/search">
		<label>
			<input class="widget-search__field" type="search" placeholder="SEARCH..." value="" name="q" aria-label="SEARCH...">
		</label>
		<input class="widget-search__submit" type="submit" value="Search">
		<input type="hidden" name="sitesearch" value="/" />
	</form>
</div>
<div class="widget-recent widget">
	<h4 class="widget__title">Recent Posts</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/post/2019-02-25-logistic/">Logistic</a></li>
			<li class="widget__item"><a class="widget__link" href="/post/2019-02-05-rl6/">RL-Diagnóstico</a></li>
			<li class="widget__item"><a class="widget__link" href="/post/2019-02-24-pca/">PCA</a></li>
			<li class="widget__item"><a class="widget__link" href="/post/2019-01-25-rl5/">RL-Multiple</a></li>
			<li class="widget__item"><a class="widget__link" href="/post/2019-01-25-rl4/">RL-Hipótesis</a></li>
		</ul>
	</div>
</div>
<div class="widget-categories widget">
	<h4 class="widget__title">Categories</h4>
	<div class="widget__content">
		<ul class="widget__list">
			<li class="widget__item"><a class="widget__link" href="/categories/development">Development</a></li>
			<li class="widget__item"><a class="widget__link" href="/categories/r">R</a></li>
		</ul>
	</div>
</div>
<div class="widget-taglist widget">
	<h4 class="widget__title">Tags</h4>
	<div class="widget__content">
		<a class="widget-taglist__link widget__link btn" href="/tags/basic-elements" title="Basic elements">Basic elements</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/classification" title="Classification">Classification</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/clustering" title="Clustering">Clustering</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/css" title="Css">Css</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/glm" title="Glm">Glm</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/html" title="Html">Html</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/logistic" title="Logistic">Logistic</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/logodds" title="Logodds">Logodds</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/mco" title="Mco">Mco</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/pca" title="Pca">Pca</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/regresi%c3%b3n-lineal" title="Regresión lineal">Regresión lineal</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/regression" title="Regression">Regression</a>
		<a class="widget-taglist__link widget__link btn" href="/tags/unsupervised-statistical-learning" title="Unsupervised statistical learning">Unsupervised statistical learning</a>
	</div>
</div>
</aside>
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2019 Estadística II.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML" async></script>
</body>
</html>