---
title: "PCA"
author: "JLMR"
date: 2019-01-28T21:14:14-05:00
categories: ["R"]
tags: ["PCA", "clustering", "classification", "unsupervised statistical learning"]
mathjax : true
menu:
  main:
    name: PCA
    weight: 12
---

```{r,echo=FALSE, message=FALSE,warning=FALSE}
library(gapminder)
library(dplyr)
library(purrr)
library(tidyr)
library(ggplot2)
library(broom)
library(ggthemes)
```

## Análisis de Componentes Principales (PCA).

##### *"Too much of anything is good for nothing!"*

#### Motivación

El método **PCA** permite reexpresar un conjunto multidimensional de datos (ej. k dimensiones) que contiene variables  **correlacionadas** ( ej. que aportan información redundante), en un subconjnto de datos de menor dimensión. 

Con **PCA** respondemos preguntas como:

¿Qué variables estan correlacionadas?

¿Podemos representar los datos de una manera más clara ej.mediante un menor número de dimensiones?

¿Qué variables tienen la mayor influencia para explicar la variación de los datos?


En este entorno  cada variable la consideramos una dimensión.

Como ejemplo de la estructura de datos  considere $x_i$ la observación del individuo *i* tomado de la variable **k**  donde i varia de **i** a **I** y k a **K**
Entonces en esta estructura la idea es reeprezar los datos a partir de  un número de  k´s. menor.

El **PCA** es  una técnica para descubrir como variables **numéricas** **"covarian"**.

La aplicación del método reduce a un subconjunto de  dimensiones (componentes principales) la información original. En  análisis multivariado  el concepto *Synthetic variable*  denota la escencia del PCA.^[ Sugerencia: Jean Paul Bencecri sobre orígenes de este vínculo entre Multivariate Analysis  Data Analysis y Principal Componentes]. 

La estructura de datos resultante maximiza la variación inherente al conjunto de datos.



Los componentes son combinaciones lineales de las variables originales, que controlan por el "ruido" (información redundante) en los datos. Nos interesa la **señal** no el **ruido**!

El **componente principal**  (Eigenvector) es una combinación lineal de las variables consideradas para predicción. Este captura la mayor pate de la variación en las dimensiones consideradas.

![](\img\pca1.jpg)

En la gráfica anterior el punto azul representa las coordenadas del valor promedio de las variables población y gasto en educación.


Note en este caso que la linea central en el primer panel  captura la dirección por la que las dos variables se relacionan, en cierto sentido este vector sintetiza la relación entre las variables pobalción y  gasto en educación para una muestra de 100 ciudades.

Si tomamos este vector y lo rotamos de tal manera que sea paralelo al eje horizontal ahora vemos una segunda fuente de variación (un segundo vector que captura otra dimensión de la  variación en los datos) correspondiente a la variación vertical (representada por la distancia entre cada observación y nuestro  primer vector). 

En este ejemplo el  plano formado por el  vector en verde lo denominamos componente principal del conjunto de datos. Note que un segundo compoente principal se forma por el vector en azul.

![](\img\pca2.jpg)


El método permite estimar la proporción de la variación que cada dimensión (componentes principales) captura, de tal forma que podemos identificar las variables de ma yor relevancia para el análisis y conocer las variables correlacionadas.

Esa variación generalmente se representa mediante un **Scree plot** ( **x** axis componentes principales, **y** axis proporción de la varianza explicada).

En la estimación de **PCA**, el monto de la variación explicada que cada componente retiene se captura por el **eigenvalor** y se expresa como cociente de la variación total en los datos.

Uno de los uso de esta técnica es la construcción de modelos que permiten hacer clasificación como el modelo de regresión logistica  (considerado parte del conjunto de GLM  (*generalized linear models*)). 

El proceso de PCA aplicado a un problema de clasificación es el siguiente:

1.-Partimos de  un conjunto de datos que  contienen k dimensiones. La técnica de  PCA permite reducir estas dimensiones al determinar los **eigenvectores** y los correspondientes  **eigenvalores (loadings)**, estos permiten determinar la proporción de la varianza explicada de cada componente con lo que se tiene un criterio para seleccionar aquellos que capturan la mayor varianza posible en el conjunto de los datos.

El número de eigenvectores a retener es una cuestión empírica que depende del escenario de análisis, si bien, un *Threshold* común es considerar aquellos pc que aportan $\geq 85\%$ de la varianza en los datos. de manera alternativa  conservar aquellos Eigenvectores con Eigenvalores  $\lambda\geq1$.^[Estre criterio , denominado regla de Kaiser, parte de la idea de que un $\lambda=1$ qeuivale a la varaición promedio aportada por un **eigenvalor** cualesquiera en el conjunto, entonces renterner aquellos que aportan mas de la variación promedio intuitivamente tiene sentido.]


2.- En el caso  supervisado la nueva estrucura de datos de menor  dimensión generada por el PCA puede ser utilizada para estimar modelos categóricos (de aprendizaje supervisado. ej tenemos una variable target para predecir) como  regresión logística,  *k nearest neighbors*, *linear discriminant Análysis* (**LDA**), etc,. 

Estos permiten determinar la probabilidad de pertenencia a una categoría determinada dado un conjunto de  característcas que describen las unides de análisis. 

El domino de aplicación es amplio. Una aproximación inicial a su uso es en la **visualización**, un proceso fundamental para comprender como se comportan nuestros datos antes de aplicar modelos de mayor complejidad. (VSM Vector Support Machine, NN Neural Networks , RF Random Forest,etc.,).

El dominio de aplicación de PCA  incluye  estudios poblacionales en areas como, migración,salud pública, indicadores socioeconómicos etc. 


### Elementos teóricos.

Antes de avanzar consideremos algunos fundamentos de algebra lineal necesarios para tener un mejor entendimiento de esta técnica:

Suponga que tiene la siguiente  matriz con dimensiones mxn   m=5, n=3

```{r, echo=FALSE}

Name<-c("A",	"B",	"C",	"D",	"E")
Age<-c(24,	50,	17,	35,	65)
Height<-c(152,	175,	160,	170,	155)
IQ<-c(108,102,	95,	97,	87)

matrix<-as.data.frame(cbind(Age, Height
                        ,IQ))
matrix

```

Entonces podemos expresar el primer vector de esta matriz como 
$$\vec{x_1}  = \begin{bmatrix} 24  \\152 \\ 108 \end{bmatrix}$$
Suponga que tenomos una matriz *A* nxn   y un vector nx1 **v**. si multiplicamos **v** por **A** Obtenemos otro vector. La matriz **A**, ha realizado una transformación sobre el vector **v**.  

![](/img/matrix.jpg)

El producto resultante, la matriz transformada, cambió el tamaño,  no la dirección del vector. 

\(\begin{bmatrix}2&3\\1&2\end{bmatrix} 
\begin{bmatrix}2\\5\end{bmatrix}=\begin{bmatrix}19\\12\end{bmatrix}\)


Recordemos la definición de Eigenvectores (vectores propios/característicos) y Eigenvalores.
**Eigenvectores**. Un vector tal que cuando son transformados por el operador resultan en un múltiplo escalar de si mismos.

En otras palabras el vector característico **v** de una transformación lineal es un vector $\neq0$ que cambia únicamente por un factor escalar cuando se le aplica la transformación lineal.  

En este contexto el escalar $\lambda$ es denominado Eigenvalor (valor propio, o característico).

\(A\vec{v.}=\lambda \vec{v.}\)

Donde **v** es un Eigenvector, $\lambda$ el Eigenvalor y A una matriz cuadrada. 

Para mayor referencia considere la definición en el   [Anton H](https://drive.google.com/file/d/1QIETnbXFvpk1GOOoFnA5l0SL7QDytN6h/view?usp=sharing):

![](\img\pca3.jpg)

La aplicación de la teoría  de vectores es particularmente útil para estudiar problemas de dinámica poblacional.  


[Sugerencia leer  Post](https://medium.com/@andrew.chamberlain/using-eigenvectors-to-find-steady-state-population-flows-cd938f124764)



**Matriz simétrica:** Una matriz $mxn$ se dice que es simétrica si $A^T =A$

Por ejemplo:

$$\begin{bmatrix}1&2&3\\2&4&5\\3&5&6\end{bmatrix}$$
En general la imagen de un vector **x** ante la   multiplicación por una matriz **A** difiere tanto en dirección como en magnitud. 

Sin emabargo, en el caso especial donde **x** es un eigenvector de **A**; la multiplicación por **A** deja la dirección sin cambio. 

Por ejemplo en   $\mathbb{R}^2$ o $\mathbb{R}^3$ la multiplicación por **A** proyecta cada **eigenvector** **x** de **A** (en caso de existir),  a lo largo de la
misma linea de **x** que atraviesa el origen.

Dependiendo del signo y magnitud del eigenvalor $\lambda$ correspondiente a **x**, la operacción $Ax=\lambda x$ comprime o expande **x** por un factor igual a $\lambda$, con una dirección inversa en el caso de que $\lambda<0$.

La siguiente figura disponible en Anton H. ilustra este efecto.

![](\img\pca4.jpg)


Note de la discusión anterior que la ecuación $Ax=\lambda x$ puede escribirse como $Ax=\lambda I x$ o bien de manera equivalente como: 

$$(\lambda I -A)x=0$$

Para que $\lambda$ sea un eigenvalor de **A**, esta ecuación debe tener una solución $\neq0$ para **x**. Lo cual ocurre **si y sólo si**  si la matriz de coeficientes  $(\lambda I -A)$ tiene un determinate $=0$.

La observación anterior se expresa por la ecuación característica:

$$det(\lambda I -A)=0$$
En palabras. Si **A** es una matriz nxn, entonces $\lambda$ es un eigen vector de **A** **si y solo si**, este satisface la **ecuación característica**. 
 
 
El proceso de cálculo  **PCA** se basa en estos fundamentos teóricos para la determinación de los **eigenvectores** y correspondientes **eigenvalores** de la matriz de datos.

Entonces los eigenvectores capturan la "dirección" principal en la que se ubican los datos en el espacio vectorial y los eigenvalores actuan como ponderadores para determinar la extensión de estos eigenvectores.
 
**Vectores ortogonales:** Dos vectores se consideran ortogonales cuando su producto punto cumple:

$$\vec{u_.}\cdot \vec{v_.}=0$$

**Teorema:**

Sea **A** una matriz simétrica. Entonces, existen **valores** propios reales $\lambda_1, \lambda_2,\lambda_3,...,\lambda_n,$  y **vectores** propios ortogonales   $\vec{v_1},\vec{v_2},\vec{v_3},...\vec{v_n}\neq 0$  
tal que $A\vec{v_i}=\lambda \vec{v_i}$ para $i=1,2,...,n$

Adicionalmente consideremos la siguientes observaciones:


1. Sea $A \in \mathbb{R}$ una matriz mXn. Entoces tanto  $A^T \cdot A$ como $A \cdot  A^T$ son simétricas.

$(AA^T)^T=(A^T)^T \cdot A^T=A \cdot A^T$

$(A^T A)^T=A^T \cdot (A^T)^T=A^T A$


2.- Sea A una matriz mxn. Entonces la matriz $AA^T$ y $A^TA$ tienen los mismos valores propios $\neq0$

Considere un vector propio  $v$ $\neq0$ de $A^TA$ con valor propio  $\lambda \neq0$ ej. $(A^TA)v=\lambda v$

Ahora si multiplicamos la matriz anterior por A tenemos $$(AA^T)(Av)=\lambda(Av)$$ Lo que implica que $Av$ es un vector propio de $A A^T$ con Eigenvalor $\lambda$. Entonces los valores característicos de $A^TA$ son los mismos que $AA^T$.

El uso de esta propuesta es importante por que permite simplificar el computo para estimar los eigenvalores por ejemplo considere una matriz de 400x3 entonces $AA^T$ es una matriz de 400X400 con 400 $\lambda$'s pero $A^TA$ tiene solo 3 $\lambda$'s, 

**Para enfatizar:**

Con la técnica de PCA determinamos los vectores propios, estos vectores capturan el espacio común a lo largo del cuál se registra la variación de los datos  y los valores propios, que dan cuenta de la magnitud de la varianza que cada vector propio explica. 

Para comprender el potencial del uso de PCA en el análisis de datos primero ubiquemos el proceso que sigue esta técnica en el esquema más amplio del análisis y en seguida haremos una serie de ejemplos comenzando con una  aplicación para una matriz de 500+ , 30.

## El proceso de cálculo. 

Primero obtenemos la media de las **m** variables en nuestra matriz. Y almacenamos estos valores en un vector en el espacio $\mathbb{R}^m$

$\vec{\mu}=\frac{1}{n-1}(\vec{x}_1+\vec{x}_2+\ldots+\vec{x}_n)$

En segundo lugar estandarizamos los datos (media cero, desviación estándar=1), este paso permite homogeneizar las escalas para evitar que las diferencias en las unidades de medida generen distorciones en el cálculo de los componentes principales. Las siguiente gráfica muestra el efecto de la estandarización (Scale).

![](/img/v.jpg)

El proceso de estandarización permite obtener una matriz  escalada:

$B=\begin{bmatrix}\vec{x}_1-\vec{\mu}|\ldots|\vec{x}_n-\vec{\mu}\end{bmatrix}$

Ahora estimamos la matriz de covarianza **S** que se define como $$\displaystyle S=\frac{1}{n-1}BB^T$$

De la discusión previa podemos ver que S es una matriz simétrica mxm.

Consideremos como ejemplo: 
$$\vec{x}_1=\begin{bmatrix}a_1\\a_2\\a_3\end{bmatrix}, \vec{x}_2=\begin{bmatrix}b_1\\b_2\\b_3\end{bmatrix}, \vec{x}_3=\begin{bmatrix}c_1\\c_2\\c_3\end{bmatrix}, \vec{\mu}=\begin{bmatrix}\mu_1\\\mu_2\\\mu_3\end{bmatrix}$$
Con matriz escalada  **B**:
$$B=\begin{bmatrix}a_1-\mu_1&b_1-\mu_1&c_1-\mu_1\\a_2-\mu_2&b_2-\mu_2&c_2-\mu_2\\a_3-\mu_3&b_3-\mu_3&c_3-\mu_3\end{bmatrix}$$
Ahora veamos  los elementos de la matriz de covarianza **S**

$$\displaystyle S_{11}=\frac{1}{n-1}((a_1-\mu_1)^2+(b_1-\mu_1)^2+(c_1-\mu_1)^2)=\mbox{ Varianza de la primera variable }$$
De manera similar los elementos $S_{22}, S_{33}$ representan la varianza de la segunda y tercera variables respectivamente.

En segiuida tenemos la covarinza entre la primera y segunda variables:

$$\displaystyle S_{12}=\frac{1}{n-1}((a_1-\mu_1)(a_2-\mu_2)+(b_1-\mu_1)(b_2-\mu_2)+(c_1-\mu_1)(c_2-\mu_2))=S_{21}$$
Y generalizando tenemos   que:

+ $S_{ii}$ es la *varianza* de la $i_{esima}$ variable. 

+ $S_{ij}$ con $i\neq j$ es la covarianza entre las variables i, j. 


El siguiente paso es aplicar el teorema descrito en la sección previa, dado que la matriz **S**  es simétrica tendrá Eigenvalores $\lambda_1\geq \lambda_2\geq \ldots\geq\lambda_m\geq 0$ y Eigenvectores ortogonales $\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_m$ Estos vectores son denominados los componentes principales de la base de datos.

Note que el segundo componente principal es una combinación lineal de las variables no correlaciondas con pc1, condición  que se deduce de la definición de vectores ortogonales. 

Paso siguiente, determiar la varianza total **T** en los datos, que es igual a la **traza**^[Traza: Suma de los elementos de la diagonal de una matriz nxn] de la matriz de **covarianza**.

$$T=\lambda_1+\lambda_2+\ldots+\lambda_m$$
¿Cómo interpretamos los resultados del PCA?

La dirección dada por el vector $\vec{v_{1}}$ $\mathbb{R}^m$ denominado el **primer componente principal** da cuenta de un monto igual a $\lambda_1$ de la varianza total T.

Es una fracción $\displaystyle\frac{\lambda_1}{T}$ de la varianza total. De igual forma, la segunda dirección principal $\vec{v}_2$,  que da cuenta de una fracción $\displaystyle\frac{\lambda_2}{T}$.

+ El vector $\vec{v}_1\in \mathbb{R}^m$ constituye  la dirección de mayor importancia en el conjunto de datos.

+ Entre las direcciones ortogonales a $\vec{v}_1$,  $\vec{v}_2$ es la dirección más significativa.

+ De forma similar entre las direcciones ortogonales a $\vec{v}_1$ y  $\vec{v}_2$,  $\vec{v}_3$ es la dirección más significativa y así sucesivamentente.

¿Que conjunto de datos tenemos como resultado del PCA? 

El procedimiento de PCA  ha generado una nueva estructura de los datos (reducción en las dimensiones) por lo que ahora  podemos representar digamos un espacio vectorial que era digamos $\mathbb{R}^{30}$ mediante una estructura de compoentes principales de menor dimensión con las observaciones del conjunto de datos conformando clusters definidos en función de los Eigenvectores.

## Práctica.


### Ejemplo 1   Iris. PCA1iris.ipynb
Ver Jupyter notebook

### Ejemplo 2   Hipotético 200 obs.  PCA2.ipynb
Ver Jupyter notebook


### Ejemplo 3 Breast cáncer data set


El propósito es determinar un subconjunto de variables que capturen la mayor cantidad de información posible (ej. que capturen la mayor variación posible).

Esta técnica permite además visualizar los vectores a lo largo de los cuales la mayor variación posible es capturada (2D).

Este subconjunto de variables consituyen una síntensis que representa los eigenvectores de la matriz original. 

[Background](http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29)
```{r,echo=FALSE}
library(corrplot)

# Reading  in the data

bc<-read.csv("https://raw.githubusercontent.com/patrickmlong/Breast-Cancer-Wisconsin-Diagnostic-DataSet/master/data.csv")

# Basic overview of data
str(bc)
names(bc)
View(bc)
```
Observamos que este **df** contine 569 obsrevaciones, 
en este caso pacientes con diagnóstico de cáncer de mama. Tenemos  30 variables. el df constituye una matriz nxm: 569x30. 

Sin embargo, es posible que en las 30 variables tenemos cierta correlación entre ellas. La técnica de **PCA** es útil en este caso por que podemos representar las 30 dimensiones mediante un subconjunto menor que capture la mayor variación posible contenida el la matriz. Esta reducción se basa en la trayectoria de los eigenvectores de la matriz original.  

Entre las variables tenemos una denominada diagnosis, esta indica el tipo de tumor benigno **B** o maligno **M**. esta variable es considerada el target para la clasificación posterior, por ejemplo mediante un modelo de regresión logística. 

#### 1. Fase EDA.

```{r}

# descriptive stats para todas las variables:
summary(bc)
diagnosis <- as.numeric(bc$diagnosis == "M")
# Cuantos casos por tipo?

count_diag<-group_by(bc,diagnosis)%>%
  summarise(total=n())

# Valor promedio de todas las variables  para cada grupo?
bc_mean<-group_by(bc,diagnosis)%>%
  summarise_all(funs(mean))


# Valor promedio de todas las variables  para cada grupo y tdo el df?
bc_std<-group_by(bc,diagnosis)%>%
  summarise_all(funs(sd))

bc_std.all<-(bc)%>%
  summarise_all((funs(sd)))

# Matriz de correlación
bc_matrix<-select(bc, -c(diagnosis,id, X))

# Rename cols para mejor interpretación.

cNames <- c("rad_m","txt_m","per_m",
                 "are_m","smt_m","cmp_m","con_m",
                 "ccp_m","sym_m","frd_m",
                 "rad_se","txt_se","per_se","are_se","smt_se",
                 "cmp_se","con_se","ccp_se","sym_se",
                 "frd_se","rad_w","txt_w","per_w",
                 "are_w","smt_w","cmp_w","con_w",
                 "ccp_w","sym_w","frd_w")


colnames(bc_matrix) <- cNames

M <- round(cor(bc_matrix), 2)
corrplot(M, diag = FALSE, method="color", order="FPC", tl.srt = 90)

```
Note el grado de correlación entre  las variables de la muestra. Soporte en favor de la decisión de usar la metodología de PCA.

### 2. Implementación. Pre proceso (Scaling)

Una pregunta escencial **previo** a la implementación de **PCA** es si debemos estandarizar (scalar) nuestra matriz. ( variables con media=0,  std dev=1).( Z-Score Normalization. *Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one.*)

Nota sobre el proceso de estandarización.

*We can think of Principle Component Analysis (PCA) as being a prime example of when normalization is important. In PCA we are interested in the components that maximize the variance. If one component (e.g. human height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds with the 'weight' axis, if those features are not scaled. As a change in height of one meter can be considered much more important than the change in weight of one kilogram, this is clearly incorrect.*


La respuesta es directa,  cuando las unidades de medida nos son homogéneas
es **necesario** escalar los datos.

Escalar los datos implica  utilizar la **matriz de correlación**. Utilizar los datos sin escalar require  utilizar la correspondiente  **matriz de  covarianza**.

De lo contrario los **eigenvectores** resultantes en el proceso de estimación  de **PCA** no aportan información consistente.(ej. la importancia de cada variable y sus unidades de medida  tendrá un impacto en la organización de los **eigenvectores** resultantes distorsionando la  varianza real explicada por cada componente).

#### 3. Estimacion de PCA usando la  matriz de correlación.

```{r, echo =FALSE}

bc.pr <- prcomp(bc_matrix, scale = TRUE, center = TRUE)
summary(bc.pr)
```

```{r}

# Set up 1 x 2 plotting grid
par(mfrow = c(1, 2))

# Calculate variability of each component
pr.var <- bc.pr$sdev ^ 2

# Variance explained by each principal component: pve
pve <- pr.var/sum(pr.var)

# Eigen values  (loadings)
round(pr.var, 2)

pr.var

# Percent variance explained
round(pve, 2)

# Cummulative percent explained
round(cumsum(pve), 2)
```

El 89% de la varianza es explicada por los primeros 6 componentes. Adicionalmente vemos que  eigenvalores>1 para los 6 pc que explican el 89% de la varianza.  Este será el criterio de selección para retenter en este caso los pc (ej . si los eigenvalores>1).





Scree  plot para mostrar la varianza explicada por cada componente.

```{r, echo=FALSE}

# Plot variance explained for each principal component

plot(pve, xlab = "Principal Component", 
     ylab = "Proporción  de la varianza explicada", 
     ylim = c(0, 1), type = "b")
```

```{r}
# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")
```

#### Visualización de los datos. Plot de Observaciones para compoenetes 1 y 2.
```{r}

plot(bc.pr$x[, c(1, 2)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC2")
legend(x="topleft", pch=1, col = c("red", "black"), legend = c("B", "M"))
```

#### Aproximación a la solución del problema como un escenario de *supervised learning*.

La variable que nos interesa predecir es *diagnosis*, ej. determinar si el tumor es maligno o benigno (variable dependiente) con base en las 30 variables independientes.

Primero aplicamos el PCA para reducir las dimensiones del df. esto nos permite clasificar a los pacientes con base en los atributos y visualizar el resultado mendiante un menor número de dimensiones.   Get jupyter notebook para mayor referencia sobre el caso de estudio en: 

https://www.kaggle.com/shravank/predicting-breast-cancer-using-pca-lda-in-r 

### Ejemplo 4 Wine Quality PCA_4.ipynb
Ver Jupyter notebook

### Ejemplo 5 emiticons Tweeter pca. 
