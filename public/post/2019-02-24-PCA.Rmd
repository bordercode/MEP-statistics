---
title: "PCA"
author: "JLMR"
date: 2019-01-28T21:14:14-05:00
categories: ["R"]
tags: ["PCA", "clustering", "classification", "unsupervised statistical learning"]
mathjax : true
menu:
  main:
    name: PCA
    weight: 12
---

```{r,echo=FALSE, message=FALSE,warning=FALSE}
library(gapminder)
library(dplyr)
library(purrr)
library(tidyr)
library(ggplot2)
library(broom)
library(ggthemes)
```

## Análisis de Componentes Principales (PCA).

El método **PCA** permite reexpresar un conjunto multidimensional de datos (ej. k dimensiones) que contiene variables altamente **correlacionadas** ( ej. que aportan información redundante), en un subconjnto de datos de menor dimensión. 

Con **PCA** respondemos preguntas como:

¿Qué variables estan correlacionadas?

¿Podemos representar los datos de una manera más clara ej.mediante un menor número de dimensiones?

¿Qué variables tienen la mayor influencia para explicar la variación de los datos?


Cada variable  considerada como una dimensión.

Sea $x_i$ la observación del individuo *i* tomado de la variable **k**  donde i varia de **i** a **I** y k a **K**

El **PCA** es  una técnica ´para descubrir como variables **numéricas** **"covarian"**.

La aplicación del método reduce a un subconjunto de  dimensiones (componentes principales) la información original. En  análisis multivariado  el concepto *Synthetic variable*  denota la escencia del PCA. Ver a Jean Paul Bencecri sobre orígenes de este vínculo entre Multivariate Analysis  DA y PCA. 

La estructura de datos resultante maximiza la variación inherente al conjunto de datos.

Los componentes son combinaciones lineales de las variables originales, que controlan por el "ruido" (información redundante) en los datos. Nos interesa la **señal** no el **ruido**!

El **componente principal** es una combinación lineal de la variables consideradas para predicción. Este captura la mayor pate de la variación en las dimensiones consideradas.

El método permite estimar la proporción de la variación que cada dimensión aporta a los componentes principales, de tal forma que podamos identificar las variables de mayor relevancia para el análisis y conocer las variables correlacionadas.

En la estimación de **PCA**, el monto de la variación explicada que cada componente retiene se captura por el **eigenvalor** y se expresa como cociente de la variación total en los datos.

Uno de os uso de esta técnica es elconstrucción de modelos que permiten hacer clasificación como el modeo de regresión logistica que vermos en las sigueintes sesiones.

El proceso de análisis aplicado de PCA a clasificación es el siguiente:

1.-Partimos de  un conjunto de datos que  contienen k dimensiones. La técnica de  PCA permite reducir estas dimensiones al determinar los **eigenvectores** y los correspondientes  eigenvalores (loadings), estos permiten determinar la proporci[onde la varianza explicada de cada compoente con lo que se tiene un criterio para seleccionar aquellos que capturan la mayor varianza posibel en el conjunto de los datos.

2.- En el caso  supervisado la nueva estrucura de datos de menor  dimensión generada por el PCA puede ser utilizada para estimar modelos categóricos como  regresión logística,  *k nearest neighbors*, *linear discriminant Análysis* (**LDA**), etc,. 

Estos permiten determinar la probabilidad de pertenencia a una categoria determinada dadas las caracterisitcas de cada observación.

Las aplicaciones son muy amplias, ej. estudios poblacionales relacionados a escenarios de salud pública, migración,etc. 

### Elementos teóricos.

Antes de avanzar consideremos algunos fundamentos de algebra lineal necesarios para tener un mejor entendimiento de esta técnica:

Suponga que tiene la sigueinte  matriz con dimensiones mxn  n=5, m=3,

```{r, echo=FALSE}

Name<-c("A",	"B",	"C",	"D",	"E")
Age<-c(24,	50,	17,	35,	65)
Height<-c(152,	175,	160,	170,	155)
IQ<-c(108,102,	95,	97,	87)

matrix<-as.data.frame(cbind(Age, Height
                        ,IQ))
matrix

```

Entonces podemos expresar el primer vector de esta matriz como 
$$\vec{x_1}  = \begin{bmatrix} 24  \\152 \\ 108 \end{bmatrix}$$
Suponga que tenomos una matriz *A* nxn   y un vector nx1 *v*. si multiplicamos *v* por *A* Obtenemos otro vector. La matriz A ha realizado una transformación sobre el vector v.  

![](/img/matrix.jpg)

Note que el producto resultante la mtriz transformada cambió el tamaño,  no la dirección del vector. 

\(\begin{bmatrix}2&3\\1&2\end{bmatrix} 
\begin{bmatrix}2\\5\end{bmatrix}=\begin{bmatrix}19\\12\end{bmatrix}\)


Recordemos la definición de Eigenvectores (vectores propios/característicos) y Eigenvalores.

**Eigenvectores**. Un vector tal que cuando son transformados por el operador resultan en un múltiplo escalar de si mismos.

En este contexto el escalar $\lambda$ es denominado Eigenvalor (valor propio, o característico).

\(Av=\lambda v\)

Donde **v** es un Eigenvector y $\lambda$ el Eigenvalor.

**Matriz simétrica:** Una matriz $mxn$ se dice que es simétrica si $A^T =A$

Por ejemplo:

$$\begin{bmatrix}1&2&3\\2&4&5\\3&5&6\end{bmatrix}$$

**Vectores ortogonales:** Dos vectores se consideran ortogonales cuando:

$\vec{u_.}\cdot \vec{v_.}=0$

**Teorema:**

Sea **A** una matriz simétrica. Entonces, existen **valores** propios reales $\lambda_1, \lambda_2,\lambda_3,...,\lambda_n,$  y **vectores** propios ortogonales $\vec{v_1},\vec{v_2},\vec{v_3},...\vec{v_n}\neq 0$  
tal que $A\vec{v_i}=\lambda \vec{v_i}$ para $i=1,2,...,n$

Adicionalmente consideremos la siguientes observaciones:


1. Sea $A \in \mathbb{R}$ una matriz mXn. Entoces tanto  $A^T \cdot A$ como $A \cdot  A^T$ son simétricas.

$(AA^T)^T=(A^T)^T \cdot A^T=A \cdot A^T$

$(A^T A)^T=A^T \cdot (A^T)^T=A^T A$


2.- Sea A una matriz mxn. Entonces la matriz $AA^T$ y $A^TA$ tienen los mismos valores propios $\neq0$

Considere un vector propio  $v$ $\neq0$ de $A^TA$ con valor propio  $\lambda \neq0$ ej. $(A^TA)v=\lambda v$

Ahora si multiplicamos la matriz anterior por A tenemos $$(AA^T)(Av)=\lambda(Av)$$ Lo que implica que $Av$ es un vector propio de $A A^T$ con Eigenvalor $\lambda$. Entonces los valores característicos de $A^TA$ son los mismos que $AA^T$.

El uso de esta propuesta es importante por que permite simplificar el computo para estimar los eigenvalores por ejemplo considere una matriz de 400x3 entonces $A^TA$ es una matriz de 400X400 con 400 $\lambda$'s pero $A^TA$ tiene solo 3 $\lambda$'s, 


Con la técnica de PCA determinamos los vectores propios, estos vectores capturan el espacio común a lo largo del cuál se registra la variación de los datos  y los valores propios, que dan cuenta de la magnitud de la varianza que cada vector propio explica. 

Primero veamos como opera esta tecnica y luego haremos una aplicación  para una matriz de 500+ , 30.

## PCA. El proceso de cálculo. 

Primero obtenemos la media de las **m** variables en nuestra matriz. Y almacenamos estos valores en un vector en el espacio $\mathbb{R}^m$

$\vec{\mu}=\frac{1}{n-1}(\vec{x}_1+\vec{x}_2+\ldots+\vec{x}_n)$

En segundo lugar estandarizamos los datos (media cero, desviación estándar=1), este paso permite homogeneizar las escalas para evitar que las diferencias en las unidaes de medida generen distorciones en el cálculo de los componentes principales. Las siguiente gráfica muestra el efecto de la estandarización (Scale).


![](/img/v.jpg)

El proceso de estandarización permite obtener una matriz  escalada:

$B=\begin{bmatrix}\vec{x}_1-\vec{\mu}|\ldots|\vec{x}_n-\vec{\mu}\end{bmatrix}$

Ahora estimamos la matriz de covarianza **S** que se define como $$\displaystyle S=\frac{1}{n-1}BB^T$$

De la discusión previa podemos ver que S es una matriz simétrica mxm.

Consideremos como ejemplo: 
$$\vec{x}_1=\begin{bmatrix}a_1\\a_2\\a_3\end{bmatrix}, \vec{x}_2=\begin{bmatrix}b_1\\b_2\\b_3\end{bmatrix}, \vec{x}_3=\begin{bmatrix}c_1\\c_2\\c_3\end{bmatrix}, \vec{\mu}=\begin{bmatrix}\mu_1\\\mu_2\\\mu_3\end{bmatrix}$$
Con matriz escalada  **B**:
$$B=\begin{bmatrix}a_1-\mu_1&b_1-\mu_1&c_1-\mu_1\\a_2-\mu_2&b_2-\mu_2&c_2-\mu_2\\a_3-\mu_3&b_3-\mu_3&c_3-\mu_3\end{bmatrix}$$
Ahora veamos  los elementos de la matriz de covarianza **S**

$$\displaystyle S_{11}=\frac{1}{n-1}((a_1-\mu_1)^2+(b_1-\mu_1)^2+(c_1-\mu_1)^2)=\mbox{ Varianza de la primera variable }$$
De manera simira los elementos $S_{22}, S_{33}$ representan la varianza de la segunda y tercera variables respectivamente.

En segiuida tenemos la covarinza entre la primera y segunda variables:

$$\displaystyle S_{12}=\frac{1}{n-1}((a_1-\mu_1)(a_2-\mu_2)+(b_1-\mu_1)(b_2-\mu_2)+(c_1-\mu_1)(c_2-\mu_2))=S_{21}$$
Y generalizando tenemos   que:

+ $S_{ii}$ es la *varianza* de la $i_{esima}$ variable. 

+ $S_{ij}$ con $i\neq j$ es la covarianza entre las variables i, j. 


El siguiente paso es aplicar el teorema descrito en la sección previa, dado que la matriz **S**  es simétrica tendrá Eigencvalores $\lambda_1\geq \lambda_2\geq \ldots\geq\lambda_m\geq 0$ y Eigenvectores ortogonales $\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_m$ Estos vectores son denominados los componentes principales de la base de datos.

Paso siguiente, determiar la varianza total **T** en los datos, que es igual a la **traza[^1] de la matriz de covarianza**.   (Traza: Suma de los elementos de la diagonal de una matriz nxn). Así mismo la traza de la matriz es igual a la suma de sus Eigenvalores:
$$T=\lambda_1+\lambda_2+\ldots+\lambda_m$$
¿Cómo interpretamos los resultados del PCA?

La dirección dada por el vector $\vec{v_{1}}$ $\mathbb{R}^m$ denominado el **primer componente principal** da cuente de un monto igual a $\lambda_1$ de la varianza total T.

Es una fracción $\displaystyle\frac{\lambda_1}{T}$ de la varianza total. De igual forma, la segunda dirección principal $\vec{v}_2$,  que da cuenta de una fracción $\displaystyle\frac{\lambda_2}{T}$.

+ El vector $\vec{v}_1\in \mathbb{R}^m$ constituye  la dirección de mayor importancia en el conjunto de datos.

+ Entre las direcciones ortogonales a $\vec{v}_1$,  $\vec{v}_2$ es la dirección más significativa.

+ De forma similar entre las direcciones ortogonales a $\vec{v}_1$ y  $\vec{v}_2$,  $\vec{v}_3$ es la dirección más significativa y así sucesivamentente.

Que conjunto de datps tenemos como resultado del PCA 

El procedimiento de PCA  ha generado una nueva estructura de los datos (reducción en las dimensiones) por lo que ahora  podemos representar digamos un espacio vectorial que era digamos $\mathbb{R}^{30}$ mediante una estructura de compoentes principales de menor dimensión con las observaciones del conjunto de datos conformando clusters definidos en función de los Eigenvectores.




```{r,echo=FALSE}
library(corrplot)

# Reading  in the data

bc<-read.csv("https://raw.githubusercontent.com/patrickmlong/Breast-Cancer-Wisconsin-Diagnostic-DataSet/master/data.csv")

# Basic overview of data
str(bc)
names(bc)
View(bc)

# descriptive stats para todas las variables:
summary(bc)
diagnosis <- as.numeric(bc$diagnosis == "M")
# Cuantos casos por tipo?

count_diag<-group_by(bc,diagnosis)%>%
  summarise(total=n())

# Valor promedio de todas las variables  para cada grupo?
bc_mean<-group_by(bc,diagnosis)%>%
  summarise_all(funs(mean))


# Valor promedio de todas las variables  para cada grupo y tdo el df?
bc_std<-group_by(bc,diagnosis)%>%
  summarise_all(funs(sd))

bc_std.all<-(bc)%>%
  summarise_all((funs(sd)))

# Matriz de correlación
bc_matrix<-select(bc, -c(diagnosis,id, X))

# Rename cols para mejor interpretación.

cNames <- c("rad_m","txt_m","per_m",
                 "are_m","smt_m","cmp_m","con_m",
                 "ccp_m","sym_m","frd_m",
                 "rad_se","txt_se","per_se","are_se","smt_se",
                 "cmp_se","con_se","ccp_se","sym_se",
                 "frd_se","rad_w","txt_w","per_w",
                 "are_w","smt_w","cmp_w","con_w",
                 "ccp_w","sym_w","frd_w")


colnames(bc_matrix) <- cNames

M <- round(cor(bc_matrix), 2)
corrplot(M, diag = FALSE, method="color", order="FPC", tl.srt = 90)

```
Note el grado de correlación entre  las variables de la muestra. Soporte en favor de la decisión de usar la metodología de PCA.
### Implementación.

Una pregunta escencial **previo** a la implementación de **PCA** es si debemos estandarizar (scalar) nuestra matriz. ( variables con media=0,  std dev=1).(Ver Z-Score Normalization. *Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one.*)

Nota sobre el proceso de estandarización.

*We can think of Principle Component Analysis (PCA) as being a prime example of when normalization is important. In PCA we are interested in the components that maximize the variance. If one component (e.g. human height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds with the 'weight' axis, if those features are not scaled. As a change in height of one meter can be considered much more important than the change in weight of one kilogram, this is clearly incorrect.*


La respuesta es directa,  cuando las unidades de medida nos son homogéneas
es **necesario** escalar los datos.

Escalar los datos implica  utilizar la **matriz de correlación**. Utilizar los datos sin escalar require  utilizar la correspondiente  **matriz de  covarianza**.

De lo contrario los **eigenvectores** resultantes en el proceso de estimación  de **PCA** no aportan información consistente.(ej. la importancia de cada variable y sus unidades de medida  tendrá un impacto en la organización de los **eigenvectores** resultantes distorsionando la  varianza real explicada por cada componente).

## Estimacion de PCA usando la  matriz de correlación.

```{r, echo =FALSE}

bc.pr <- prcomp(bc_matrix, scale = TRUE, center = TRUE)
summary(bc.pr)
```
```{r,echo=FALSE}

# Set up 1 x 2 plotting grid
par(mfrow = c(1, 2))

# Calculate variability of each component
pr.var <- bc.pr$sdev ^ 2

# Variance explained by each principal component: pve
pve <- pr.var/sum(pr.var)


# Eigen values  (loadings)
round(pr.var, 2)

pr.var

# Percent variance explained
round(pve, 2)

# Cummulative percent explained
round(cumsum(pve), 2)
```

####El 89% de la varianza es explicada por los primeros 6 componentes. Adicionalmente vemos que  eigenvalores>1 para los 6 pc que explican el 89% de la varianza.  Este serpa el criterio de selección para retenter en este caso los pc (ej . si los eigenvalores>1).





Scree  plot para mostrar la varianza explicada por cada componente.

```{r, echo=FALSE}

# Plot variance explained for each principal component

plot(pve, xlab = "Principal Component", 
     ylab = "Proporción  de la varianza explicada", 
     ylim = c(0, 1), type = "b")
```

```{r}
# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")
```

## plot de Observaciones para compoenetes 1 y 2.
```{r}

plot(bc.pr$x[, c(1, 2)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC2")
legend(x="topleft", pch=1, col = c("red", "black"), legend = c("B", "M"))
```

