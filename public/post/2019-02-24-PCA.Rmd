---
title: "PCA"
author: "JLMR"
date: 2019-01-28T21:14:14-05:00
categories: ["R"]
tags: ["PCA", "clustering", "classification", "unsupervised statistical learning"]
mathjax : true
menu:
  main:
    name: PCA
    weight: 12
---

```{r,echo=FALSE, message=FALSE,warning=FALSE}
library(dplyr)
library(ggplot2)
library(plotly)
library(scales)
library(corrplot)
```

## Análisis de Componentes Principales (PCA).


#### Motivación

El método **PCA** permite reexpresar un conjunto multidimensional de datos (ej. k dimensiones) que contiene variables  **correlacionadas** ( ej. que aportan información redundante), en un subconjnto de datos de menor dimensión. 

Con **PCA** respondemos preguntas como:

¿Qué variables estan correlacionadas?

¿Podemos representar los datos de una manera más clara ej.mediante un menor número de dimensiones?

¿Qué variables tienen la mayor influencia para explicar la variación de los datos?


En este entorno  cada variable la consideramos una dimensión.

Como ejemplo de la estructura de datos  considere $x_i$ la observación del individuo *i* tomado de la variable **k**  donde i varia de **i** a **I** y k a **K**
Entonces en esta estructura la idea es reeprezar los datos a partir de  un número de  k´s. menor.

El **PCA** es  una técnica para descubrir como variables **numéricas** **"covarian"**.

La aplicación del método reduce a un subconjunto de  dimensiones (componentes principales) la información original. En  análisis multivariado  el concepto *Synthetic variable*  denota la escencia del PCA.^[ Sugerencia: Jean Paul Bencecri sobre orígenes de este vínculo entre Multivariate Analysis  Data Analysis y Principal Componentes]. 

La estructura de datos resultante maximiza la variación inherente al conjunto de datos.



Los componentes son combinaciones lineales de las variables originales, que controlan por el "ruido" (información redundante) en los datos. Nos interesa la **señal** no el **ruido**!

El **componente principal**  (Eigenvector) es una combinación lineal de las variables consideradas para predicción. Este captura la mayor pate de la variación en las dimensiones consideradas.

![](\img\pca1.jpg)

En la gráfica anterior el punto azul representa las coordenadas del valor promedio de las variables población y gasto en educación.


Note en este caso que la linea central en el primer panel  captura la dirección por la que las dos variables se relacionan, en cierto sentido este vector sintetiza la relación entre las variables pobalción y  gasto en educación para una muestra de 100 ciudades.

Si tomamos este vector y lo rotamos de tal manera que sea paralelo al eje horizontal ahora vemos una segunda fuente de variación (un segundo vector que captura otra dimensión de la  variación en los datos) correspondiente a la variación vertical (representada por la distancia entre cada observación y nuestro  primer vector). 

En este ejemplo el  plano formado por el  vector en verde lo denominamos componente principal del conjunto de datos. Note que un segundo compoente principal se forma por el vector en azul.

![](\img\pca2.jpg)


El método permite estimar la proporción de la variación que cada dimensión (componentes principales) captura, de tal forma que podemos identificar las variables de ma yor relevancia para el análisis y conocer las variables correlacionadas.

Esa variación generalmente se representa mediante un **Scree plot** ( **x** axis componentes principales, **y** axis proporción de la varianza explicada).

En la estimación de **PCA**, el monto de la variación explicada que cada componente retiene se captura por el **eigenvalor** y se expresa como cociente de la variación total en los datos.

Uno de los uso de esta técnica es la construcción de modelos que permiten hacer clasificación como el modelo de regresión logistica  (considerado parte del conjunto de GLM  (*generalized linear models*)). 

El proceso de PCA aplicado a un problema de clasificación es el siguiente:

1.-Partimos de  un conjunto de datos que  contienen k dimensiones. La técnica de  PCA permite reducir estas dimensiones al determinar los **eigenvectores** y los correspondientes  **eigenvalores (loadings)**, estos permiten determinar la proporción de la varianza explicada de cada componente con lo que se tiene un criterio para seleccionar aquellos que capturan la mayor varianza posible en el conjunto de los datos.

El número de eigenvectores a retener es una cuestión empírica que depende del escenario de análisis, si bien, un *Threshold* común es considerar aquellos pc que aportan $\geq 85\%$ de la varianza en los datos. de manera alternativa  conservar aquellos Eigenvectores con Eigenvalores  $\lambda\geq1$.^[Estre criterio , denominado regla de Kaiser, parte de la idea de que un $\lambda=1$ qeuivale a la varaición promedio aportada por un **eigenvalor** cualesquiera en el conjunto, entonces renterner aquellos que aportan mas de la variación promedio intuitivamente tiene sentido.]


2.- En el caso  supervisado la nueva estrucura de datos de menor  dimensión generada por el PCA puede ser utilizada para estimar modelos categóricos (de aprendizaje supervisado. ej tenemos una variable target para predecir) como  regresión logística,  *k nearest neighbors*, *linear discriminant Análysis* (**LDA**), etc,. 

Estos permiten determinar la probabilidad de pertenencia a una categoría determinada dado un conjunto de  característcas que describen las unides de análisis. 

El domino de aplicación es amplio. Una aproximación inicial a su uso es en la **visualización**, un proceso fundamental para comprender como se comportan nuestros datos antes de aplicar modelos de mayor complejidad. (VSM Vector Support Machine, NN Neural Networks , RF Random Forest,etc.,).

El dominio de aplicación de PCA  incluye  estudios poblacionales en areas como, migración,salud pública, indicadores socioeconómicos etc. 


### Elementos teóricos.

Antes de avanzar consideremos algunos fundamentos de algebra lineal necesarios para tener un mejor entendimiento de esta técnica:

Suponga que tiene la siguiente  matriz con dimensiones mxn   m=5, n=3

```{r, echo=FALSE}

Name<-c("A",	"B",	"C",	"D",	"E")
Age<-c(24,	50,	17,	35,	65)
Height<-c(152,	175,	160,	170,	155)
IQ<-c(108,102,	95,	97,	87)

matrix<-as.data.frame(cbind(Age, Height
                        ,IQ))
matrix

```

Entonces podemos expresar el primer vector de esta matriz como 
$$\vec{x_1}  = \begin{bmatrix} 24  \\152 \\ 108 \end{bmatrix}$$
Suponga que tenomos una matriz *A* nxn   y un vector nx1 **v**. si multiplicamos **v** por **A** Obtenemos otro vector. La matriz **A**, ha realizado una transformación sobre el vector **v**.  

![](/img/matrix.jpg)

El producto resultante, la matriz transformada, cambió el tamaño,  no la dirección del vector. 

\(\begin{bmatrix}2&3\\1&2\end{bmatrix} 
\begin{bmatrix}2\\5\end{bmatrix}=\begin{bmatrix}19\\12\end{bmatrix}\)


Recordemos la definición de Eigenvectores (vectores propios/característicos) y Eigenvalores.
**Eigenvectores**. Un vector tal que cuando son transformados por el operador resultan en un múltiplo escalar de si mismos.

En otras palabras el vector característico **v** de una transformación lineal es un vector $\neq0$ que cambia únicamente por un factor escalar cuando se le aplica la transformación lineal.  

En este contexto el escalar $\lambda$ es denominado Eigenvalor (valor propio, o característico).

\(A\vec{v.}=\lambda \vec{v.}\)

Donde **v** es un Eigenvector, $\lambda$ el Eigenvalor y A una matriz cuadrada. 

Para mayor referencia considere la definición en el   [Anton H](https://drive.google.com/file/d/1QIETnbXFvpk1GOOoFnA5l0SL7QDytN6h/view?usp=sharing):

![](\img\pca3.jpg)

La aplicación de la teoría  de vectores es particularmente útil para estudiar problemas de dinámica poblacional.  


[Sugerencia leer  Post](https://medium.com/@andrew.chamberlain/using-eigenvectors-to-find-steady-state-population-flows-cd938f124764)



**Matriz simétrica:** Una matriz $mxn$ se dice que es simétrica si $A^T =A$

Por ejemplo:

$$\begin{bmatrix}1&2&3\\2&4&5\\3&5&6\end{bmatrix}$$
En general la imagen de un vector **x** ante la   multiplicación por una matriz **A** difiere tanto en dirección como en magnitud. 

Sin emabargo, en el caso especial donde **x** es un eigenvector de **A**; la multiplicación por **A** deja la dirección sin cambio. 

Por ejemplo en   $\mathbb{R}^2$ o $\mathbb{R}^3$ la multiplicación por **A** proyecta cada **eigenvector** **x** de **A** (en caso de existir),  a lo largo de la
misma linea de **x** que atraviesa el origen.

Dependiendo del signo y magnitud del eigenvalor $\lambda$ correspondiente a **x**, la operacción $Ax=\lambda x$ comprime o expande **x** por un factor igual a $\lambda$, con una dirección inversa en el caso de que $\lambda<0$.

La siguiente figura disponible en Anton H. ilustra este efecto.

![](\img\pca4.jpg)


Note de la discusión anterior que la ecuación $Ax=\lambda x$ puede escribirse como $Ax=\lambda I x$ o bien de manera equivalente como: 

$$(\lambda I -A)x=0$$

Para que $\lambda$ sea un eigenvalor de **A**, esta ecuación debe tener una solución $\neq0$ para **x**. Lo cual ocurre **si y sólo si**  si la matriz de coeficientes  $(\lambda I -A)$ tiene un determinate $=0$.

La observación anterior se expresa por la ecuación característica:

$$det(\lambda I -A)=0$$
En palabras. Si **A** es una matriz nxn, entonces $\lambda$ es un eigen valor de **A** **si y solo si**, este satisface la **ecuación característica**. 
 
 
El proceso de cálculo  **PCA** se basa en estos fundamentos teóricos para la determinación de los **eigenvectores** y correspondientes **eigenvalores** de la matriz de datos.

Entonces los eigenvectores capturan la "dirección" principal en la que se ubican los datos en el espacio vectorial y los eigenvalores actuan como ponderadores para determinar la extensión de estos eigenvectores.
 
**Vectores ortogonales:** Dos vectores se consideran ortogonales cuando su producto punto cumple:

$$\vec{u_.}\cdot \vec{v_.}=0$$

**Teorema:**

Sea **A** una matriz simétrica. Entonces, existen **valores** propios reales $\lambda_1, \lambda_2,\lambda_3,...,\lambda_n,$  y **vectores** propios ortogonales   $\vec{v_1},\vec{v_2},\vec{v_3},...\vec{v_n}\neq 0$  
tal que $A\vec{v_i}=\lambda \vec{v_i}$ para $i=1,2,...,n$

Adicionalmente consideremos la siguientes observaciones:


1. Sea $A \in \mathbb{R}$ una matriz mXn. Entoces tanto  $A^T \cdot A$ como $A \cdot  A^T$ son simétricas.

$(AA^T)^T=(A^T)^T \cdot A^T=A \cdot A^T$

$(A^T A)^T=A^T \cdot (A^T)^T=A^T A$


2.- Sea A una matriz mxn. Entonces la matriz $AA^T$ y $A^TA$ tienen los mismos valores propios $\neq0$

Considere un vector propio  $v$ $\neq0$ de $A^TA$ con valor propio  $\lambda \neq0$ ej. $(A^TA)v=\lambda v$

Ahora si multiplicamos la matriz anterior por A tenemos $$(AA^T)(Av)=\lambda(Av)$$ Lo que implica que $Av$ es un vector propio de $A A^T$ con Eigenvalor $\lambda$. Entonces los valores característicos de $A^TA$ son los mismos que $AA^T$.

El uso de esta propuesta es importante por que permite simplificar el computo para estimar los eigenvalores por ejemplo considere una matriz de 400x3 entonces $AA^T$ es una matriz de 400X400 con 400 $\lambda$'s pero $A^TA$ tiene solo 3 $\lambda$'s, 

**Para enfatizar:**

Con la técnica de PCA determinamos los vectores propios, estos vectores capturan el espacio común a lo largo del cuál se registra la variación de los datos  y los valores propios, que dan cuenta de la magnitud de la varianza que cada vector propio explica. 

Para comprender el potencial del uso de PCA en el análisis de datos primero ubiquemos el proceso que sigue esta técnica en el esquema más amplio del análisis y en seguida haremos una serie de ejemplos comenzando con una  aplicación para una matriz de 500+ , 30.

## El proceso de cálculo. 

Primero obtenemos la media de las **m** variables en nuestra matriz. Y almacenamos estos valores en un vector en el espacio $\mathbb{R}^m$

$\vec{\mu}=\frac{1}{n-1}(\vec{x}_1+\vec{x}_2+\ldots+\vec{x}_n)$

En segundo lugar estandarizamos los datos (media cero, desviación estándar=1), este paso permite homogeneizar las escalas para evitar que las diferencias en las unidades de medida generen distorciones en el cálculo de los componentes principales. Las siguiente gráfica muestra el efecto de la estandarización (Scale).

![](/img/v.jpg)

El proceso de estandarización permite obtener una matriz  escalada:

$B=\begin{bmatrix}\vec{x}_1-\vec{\mu}|\ldots|\vec{x}_n-\vec{\mu}\end{bmatrix}$

Ahora estimamos la matriz de covarianza **S** que se define como $$\displaystyle S=\frac{1}{n-1}BB^T$$

De la discusión previa podemos ver que S es una matriz simétrica mxm.

Consideremos como ejemplo: 
$$\vec{x}_1=\begin{bmatrix}a_1\\a_2\\a_3\end{bmatrix}, \vec{x}_2=\begin{bmatrix}b_1\\b_2\\b_3\end{bmatrix}, \vec{x}_3=\begin{bmatrix}c_1\\c_2\\c_3\end{bmatrix}, \vec{\mu}=\begin{bmatrix}\mu_1\\\mu_2\\\mu_3\end{bmatrix}$$
Con matriz escalada  **B**:
$$B=\begin{bmatrix}a_1-\mu_1&b_1-\mu_1&c_1-\mu_1\\a_2-\mu_2&b_2-\mu_2&c_2-\mu_2\\a_3-\mu_3&b_3-\mu_3&c_3-\mu_3\end{bmatrix}$$
Ahora veamos  los elementos de la matriz de covarianza **S**

$$\displaystyle S_{11}=\frac{1}{n-1}((a_1-\mu_1)^2+(b_1-\mu_1)^2+(c_1-\mu_1)^2)=\mbox{ Varianza de la primera variable }$$
De manera similar los elementos $S_{22}, S_{33}$ representan la varianza de la segunda y tercera variables respectivamente.

En seguida tenemos la covarinza entre la primera y segunda variables:

$$\displaystyle S_{12}=\frac{1}{n-1}((a_1-\mu_1)(a_2-\mu_2)+(b_1-\mu_1)(b_2-\mu_2)+(c_1-\mu_1)(c_2-\mu_2))=S_{21}$$
Y generalizando tenemos   que:

+ $S_{ii}$ es la *varianza* de la $i_{esima}$ variable. 

+ $S_{ij}$ con $i\neq j$ es la covarianza entre las variables i, j. 


El siguiente paso es aplicar el teorema descrito en la sección previa, dado que la matriz **S**  es simétrica tendrá Eigenvalores $\lambda_1\geq \lambda_2\geq \ldots\geq\lambda_m\geq 0$ y Eigenvectores ortogonales $\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_m$ Estos vectores son denominados los componentes principales de la base de datos.

Note que el segundo componente principal es una combinación lineal de las variables no correlaciondas con pc1, condición  que se deduce de la definición de vectores ortogonales. 

Paso siguiente, determiar la varianza total **T** en los datos, que es igual a la **traza**^[Traza: Suma de los elementos de la diagonal de una matriz nxn] de la matriz de **covarianza**.

$$T=\lambda_1+\lambda_2+\ldots+\lambda_m$$
¿Cómo interpretamos los resultados del PCA?

La dirección dada por el vector $\vec{v_{1}}$ $\mathbb{R}^m$ denominado el **primer componente principal** da cuenta de un monto igual a $\lambda_1$ de la varianza total T.

Es una fracción $\displaystyle\frac{\lambda_1}{T}$ de la varianza total. De igual forma, la segunda dirección principal $\vec{v}_2$,  que da cuenta de una fracción $\displaystyle\frac{\lambda_2}{T}$.

+ El vector $\vec{v}_1\in \mathbb{R}^m$ constituye  la dirección de mayor importancia en el conjunto de datos.

+ Entre las direcciones ortogonales a $\vec{v}_1$,  $\vec{v}_2$ es la dirección más significativa.

+ De forma similar entre las direcciones ortogonales a $\vec{v}_1$ y  $\vec{v}_2$,  $\vec{v}_3$ es la dirección más significativa y así sucesivamentente.

¿Que conjunto de datos tenemos como resultado del PCA? 

El procedimiento de PCA  ha generado una nueva estructura de los datos (reducción en las dimensiones) por lo que ahora  podemos representar digamos un espacio vectorial que era digamos $\mathbb{R}^{30}$ mediante una estructura de compoentes principales de menor dimensión con las observaciones del conjunto de datos conformando clusters definidos en función de los Eigenvectores.

## Práctica.


### Ejemplo 1   Iris. PCA1iris.ipynb
Ver Jupyter notebook

### Ejemplo 2   Hipotético 200 obs.  PCA2.ipynb
Ver Jupyter notebook


### Ejemplo 3 Breast cáncer data set


El propósito es determinar un subconjunto de variables que capturen la mayor cantidad de información posible (ej. que capturen la mayor variación posible).

Esta técnica permite además visualizar los vectores a lo largo de los cuales la mayor variación posible es capturada (2D).

Este subconjunto de variables consituyen una síntensis que representa los eigenvectores de la matriz original. 

[Background](http://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+%28diagnostic%29)

```{r,echo=FALSE}

# Reading  in the data

bc<-read.csv("https://raw.githubusercontent.com/patrickmlong/Breast-Cancer-Wisconsin-Diagnostic-DataSet/master/data.csv")

# Basic overview of data
str(bc)
names(bc)
View(bc)
```
Observamos que este **df** contine 569 obsrevaciones, 
en este caso pacientes con diagnóstico de cáncer de mama. Tenemos  30 variables. el df constituye una matriz nxm: 569x30. 

Sin embargo, es posible que en las 30 variables tenemos cierta correlación entre ellas. La técnica de **PCA** es útil en este caso por que podemos representar las 30 dimensiones mediante un subconjunto menor que capture la mayor variación posible contenida el la matriz. Esta reducción se basa en la trayectoria de los eigenvectores de la matriz original.  

Entre las variables tenemos una denominada diagnosis, esta indica el tipo de tumor benigno **B** o maligno **M**. esta variable es considerada el target para la clasificación posterior, por ejemplo mediante un modelo de regresión logística. 

#### 1. Fase EDA.

```{r}

# descriptive stats para todas las variables:

summary(bc)
diagnosis <- as.numeric(bc$diagnosis == "M")

# Cuantos casos por tipo?

count_diag<-group_by(bc,diagnosis)%>%
summarize(total=n())

# Valor promedio de todas las variables  para cada grupo?
bc_mean<-group_by(bc,diagnosis)%>%
  summarise_all(funs(mean))


# Valor promedio de todas las variables  para cada grupo y tdo el df?
bc_std<-group_by(bc,diagnosis)%>%
  summarise_all(funs(sd))

bc_std.all<-(bc)%>%
  summarise_all((funs(sd)))

# Matriz de correlación
bc_matrix<-select(bc, -c(diagnosis,id, X))

# Rename cols para mejor interpretación.

cNames <- c("rad_m","txt_m","per_m",
                 "are_m","smt_m","cmp_m","con_m",
                 "ccp_m","sym_m","frd_m",
                 "rad_se","txt_se","per_se","are_se","smt_se",
                 "cmp_se","con_se","ccp_se","sym_se",
                 "frd_se","rad_w","txt_w","per_w",
                 "are_w","smt_w","cmp_w","con_w",
                 "ccp_w","sym_w","frd_w")


colnames(bc_matrix) <- cNames

M <- round(cor(bc_matrix), 2)
corrplot(M, diag = FALSE, method="color", order="FPC", tl.srt = 90)

```
Note el grado de correlación entre  las variables de la muestra. Soporte en favor de la decisión de usar la metodología de PCA.

### 2. Implementación. Pre proceso (Scaling)

Una pregunta escencial **previo** a la implementación de **PCA** es si debemos estandarizar (scalar) nuestra matriz. ( variables con media=0,  std dev=1).( Z-Score Normalization. *Standardization involves rescaling the features such that they have the properties of a standard normal distribution with a mean of zero and a standard deviation of one.*)

Nota sobre el proceso de estandarización.

*We can think of Principle Component Analysis (PCA) as being a prime example of when normalization is important. In PCA we are interested in the components that maximize the variance. If one component (e.g. human height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds with the 'weight' axis, if those features are not scaled. As a change in height of one meter can be considered much more important than the change in weight of one kilogram, this is clearly incorrect.*


La respuesta es directa,  cuando las unidades de medida nos son homogéneas
es **necesario** escalar los datos.

Escalar los datos implica  utilizar la **matriz de correlación**. Utilizar los datos sin escalar require  utilizar la correspondiente  **matriz de  covarianza**.

De lo contrario los **eigenvectores** resultantes en el proceso de estimación  de **PCA** no aportan información consistente.(ej. la importancia de cada variable y sus unidades de medida  tendrá un impacto en la organización de los **eigenvectores** resultantes distorsionando la  varianza real explicada por cada componente).

#### 3. Estimacion de PCA usando la  matriz de correlación.

```{r, echo =FALSE}
bc.pr <- prcomp(bc_matrix, scale = TRUE, center = TRUE)
summary(bc.pr)

```

```{r}

# Set up 1 x 2 plotting grid
par(mfrow = c(1, 2))

# Calculate variability of each component
pr.var <- bc.pr$sdev ^ 2

# Variance explained by each principal component: pve
pve <- pr.var/sum(pr.var)

# Eigen values  (loadings)
round(pr.var, 2)

pr.var

# Percent variance explained
round(pve, 2)

# Cummulative percent explained
round(cumsum(pve), 2)
```

El 89% de la varianza es explicada por los primeros 6 componentes. Adicionalmente vemos que  eigenvalores>1 para los 6 pc que explican el 89% de la varianza.  Este será el criterio de selección para retenter en este caso los pc (ej . si los eigenvalores>1).





Scree  plot para mostrar la varianza explicada por cada componente.

```{r, echo=FALSE}

# Plot variance explained for each principal component

plot(pve, xlab = "Principal Component", 
     ylab = "Proporción  de la varianza explicada", 
     ylim = c(0, 1), type = "b")
```

```{r}
# Plot cumulative proportion of variance explained
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")
```

#### Visualización de los datos. Plot de Observaciones para componentes 1 y 2.

```{r}

plot(bc.pr$x[, c(1, 2)], col = (diagnosis + 1), 
     xlab = "PC1", ylab = "PC2")
legend(x="topleft", pch=1, col = c("red", "black"), legend = c("B", "M"))
```

#### Aproximación a la solución del problema como un escenario de *supervised learning*.

La variable que nos interesa predecir es *diagnosis*, ej. determinar si el tumor es maligno o benigno (variable dependiente) con base en las 30 variables independientes.

Primero aplicamos el PCA para reducir las dimensiones del df. esto nos permite clasificar a los pacientes con base en los atributos y visualizar el resultado mendiante un menor número de dimensiones.   Get jupyter notebook para mayor referencia sobre el caso de estudio en: 

https://www.kaggle.com/shravank/predicting-breast-cancer-using-pca-lda-in-r 


### Ejemplo 4  Calificaciones de posgrado.

En el siguiente ejemplo considere un conjunto de calificaciones para  alumnos de posgrado en dos asignaturas. El objetivo es: a) obtener los componentes principales y eigenvalores b) determinar la proporción de la varianza que explican c) representar los eigenvectores graficamente.


La base de datos contiene  100 observaciones correspondientes a calificaciones en las asignaturas de historia y geografia. La matriz conformada por los datos tiene dimensiones 100x2.

En este ejemplo la intención es enfatizar la estructura conceptual del algoritmo PCA. Note que ya de hecho se tiene 2 dimensiones que facilmente permiten tener una aproximación visual al comportamiento y correlación de los datos, no obstante, la aplicación de PCA permite reforzar los coceptos involucrados en la estimación del  algoritmo. 


### 1. Leemos la base de datos.

```{r, warning=FALSE, message=FALSE}

datos<-readRDS("datos.rds")
getwd()
names(datos)
str(datos)
head(datos)
```

#### 1.1 Visualización  y estadísticas descriptivas.

```{r, warning=FALSE, message=FALSE}


p<-ggplot(datos, aes(historia,geografía))+
  geom_point(color="deepskyblue4", alpha = 0.5)+
  theme_bw()+
  labs(title="Calificaciones", x="Historia (0-100)", y="Geografía (0-100)")

ggplotly(p)

summary(datos)

```

```{r}

d <- mutate(datos,mh=mean(historia), mg=mean(geografía),historia=historia-mh,geografía=geografía-mg)%>%
  select(-c(mh,mg))

plt<-ggplot(d,aes(historia,geografía))+
  geom_point()+
  geom_point(color="red", alpha = 0.5)+
  theme_bw()+
  labs(title="Calificaciones (Escala 0-100)", x="Historia", y="Geografía ")+
  xlim(c(-30,30))

ggplotly(plt)

```

###  Determinación de matriz de covarianza. Estimación de eigenvectores (CP) y eigenvalores.

```{r}
mcov <- cov(d)
eigenv <- eigen(mcov)
colnames(eigenv$vectors) <-c("PC1","PC")
rownames(eigenv$vectors) <- c("Historia","Heografía")
eigenv


pr <- prcomp(d,  center = TRUE)
summary(pr)
pr
# Hemos obtenido los eigenvalores (Standard deviations^2) y la matrz de eigenvectores  (rotation). 

# Eigenvalores
pr.var <- pr$sdev ^ 2
round(pr.var, 2)

# Proporción Variance explained by each principal component: pve
pve <- pr.var/sum(pr.var)
round(pve, 2)

# Acumulado
round(cumsum(pve), 3)

```

Note que cada componente principal es una combinación lineal de las variables en los datos. ej. CP1 es una combinación lineal de las variables historia y geografía. 

El componente principal 1 captura el $97.5\%$ de la variación de los datos. 

Note: El algoritmo PCA aporta la toda la información requerida (eigenvalores  std^2 ), eigenvectores (rotation), valores de la matriz original reproyectados con el espacio vectorial de los componentes principales (scores= loadings*datos originales). 

#### Comprobando que los eigenvalores capturan varianza total en los datos. 
```{r}
sum(eigenv$values)
var(d[,1]) + var(d[,2])
```
##### Graficamos los eigenvectores.

```{r}
# Representamos cada los eigebvectores

pc1 <- pr$rotation[1,1]/pr$rotation[2,1]
pc2 <- pr$rotation[1,2]/pr$rotation[2,2]


plt<-ggplot(d,aes(historia,geografía))+
  geom_point()+
  geom_point(color="red", alpha = 0.5)+
  theme_bw()+
  labs(title="Calificaciones (Escala 0-100)", x="Historia", y="Geografía ")+
  xlim(c(-30,30))+
  geom_abline(intercept = 0, slope = pc1, color="deepskyblue4")+
  geom_abline(intercept = 0, slope = pc2, color="orange")+
  annotate(geom="text", x=24, y=11, label="(-0.710,-0.695)",
              color="deepskyblue4")+
  annotate(geom="text", x=-24, y=11, label="(0.695,-0.719)",
              color="orange")
  
ggplotly(plt)

```
Note que son ortogonales (perpendiculares). 


```{r}

# Plot de la varianza explicada por cada componente

plot(pve, xlab = "Principal Component", 
     ylab = "Proporción  de la varianza explicada", 
 ylim = c(0, 1), type = "b")

```


```{r}
# Plot proporción acumulada.
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")
```

#### Proyección de los datos al nuevo espacio vectorial conformado por los compornetes principales.
```{r}
# Multiplicamos  los datos (estandarizados) por la matriz de componentes principales.

loadings <- pr$rotation
scores <- pr$x
scores <- as.data.frame(scores)
```

```{r}
plt2<-ggplot(scores,aes(PC1,PC2))+
         geom_point(color="darkblue", alpha=.5)+
         labs(title="Scores.Datos reproyectados con EigenVectores / PCs")+
   geom_abline(intercept = 0, slope = 0, color="yellow")+
  geom_abline(intercept = 0, slope = 90, color="red")+
  theme_light()+
  ylim(-10,10)+
  xlim(-30,30)+
  labs(x="PC1  97.5% var",y="PC2 2.5% var")
  
ggplotly(plt2)
```

#### Representación de scores y variables con Biplot
Creamos una representación gráfica con los scores (datos reproyectados *puntos*) y las variables (**vectrores**) en las misma grafica ("bi").

```{r, fig.width=6, fig.height=6, message=FALSE, warning=FALSE}
library(ggbiplot)

b2<-ggbiplot(pr, circle = TRUE)+
    theme_light()+
  geom_point(color="gray")+
  geom_abline(intercept = 0, slope = 0, color="blue")+
  geom_abline(intercept = 0, slope = 90, color="lightgreen")+
  theme_light()

b2

```

Interpretación: Las variables se encuentran representadas como vectores. En este caso la dirección en la que apuntan indica la correlación positiva entre las variables de la matriz.

Cada punto corresponde al número de la observación en nuestra base de datos ortiginal. Ej. tenemos 100 obs, tenemos 100 datos en el biplot y las coordenadas corresponden con la ubicación en el espacio vectorial generado por los componentes princilaes.

En este caso podriamos usar unicamente una sola dimensión para explicar el desempeño de los alumnos en este grupo, (ej. el primer componente captura el $97.5\%$ de la variación total en los datos).

Note el círculo reresenta  varianza de 1.5 en cada dirección.

### Ejemplo 5  Nacimientos México 2016

El siguiente ejemplo considera la base de datos de nacimientos en México para 2017. La matriz contiene  236 774 observaciones (registros de nacimientos) para las siguientes variables:


GESTACH	Semanas de gestación	Rango aceptable 13 a 42 semanas

TALLAH 	Talla de recién nacido	Rango aceptable 7 a 60 centímetros

PESOH	 largo	4	Peso del recién nacido	Rango aceptable 20 a 6000 gramos, 

APGARH	Calificación obtenida  del recién nacido a los cinco minutos del nacimiento	Entre 0 a 10, 

SILVERMAN		Calificación obtenida  del recién nacido al momento del nacimiento	Entre 10 a 0

Y la variable categórica target con valores 0,1, Donde 0 denota nacimiento normal y 1 para indicar alguna anomalía o padecimiento.

El **objetivo** es aplicar el algoritmo  **PCA** para explorar si es posible explicar la variación en los datos apartir de una estructura con menores dimensiones.

¿Con cuantos componentes principales podemos explicar el $70\%$ de la variación?

Adicionalmene determinar que variables estan correlacionadas.

[Fuente datos](http://dgis.salud.gob.mx/contenidos/basesdedatos/std_nacimientos_gobmx.html)

##### Leemos los datos.

```{r warning=FALSE, message=FALSE}
df<-readRDS("nac16set.rds")

str(df)
names(df)

head(df,5)
```

```{r warning=FALSE, message=FALSE}
## Descriptivas

summary(df)

# Eliminamos valores inválidos. ej. 99 0 9999.

df<-filter(df,edad_madre!=999|apgarh!=99,silverman!=99,tallah!=99,gestach!=99,pesoh!=9999)

# Valor promedio de todas las variables  para cada grupo?

df_mean<-group_by(df,target)%>%
  summarise_all(list(mean))

df_mean

df<-mutate(df,target=as.factor(target))

p<-ggplot(df,aes(pesoh,fill=target))+
  geom_histogram(position= "identity", alpha=0.4)+
  theme_light()+
  labs(fill="Anomalia", y="nacimientos", x="Peso (g)")+
scale_fill_discrete(labels=c("No", "Sí"))
  p   

```

```{r warning=FALSE, message=FALSE}
# Matriz de correlación

df_matrix<-select(df, -c(target))

M <- round(cor(df_matrix), 2)

corrplot(M, diag = FALSE, method="color", order="FPC", tl.srt = 90)
```


#### 3. Estimación de PCA.

```{r, warning=FALSE, message=FALSE}

df.cp <- prcomp(df_matrix, scale = TRUE, center = TRUE)
df.cp

```

```{r warning=FALSE, message=FALSE}

#  Parámetros para graficar.
par(mfrow = c(1, 2))

# # Eigenvalores  (loadings). Varianza en cada componente.
cp.var <- df.cp$sdev ^ 2
round(cp.var, 2)
cp.var


# Proporción de la varianza de cada componente
pve <- cp.var/sum(cp.var)
round(pve, 2)


# Porcentaje Acumulado explicado.
round(cumsum(pve), 2)
```

El 77% de la varianza es explicada por los primeros 3 componentes. Los tres primeros $\lambda\geq1$, acorde al  criterio de Kaiser podriamos mantener 3  cp.

#### Scree  plot para mostrar la varianza explicada por cada componente.

```{r, warning=FALSE, message=FALSE}

# Plot variance explained para cada  componente principal 

plot(pve, xlab = "Principal Component", 
     ylab = "Pr varianza explicada", 
     ylim = c(0, 1), type = "b")
```

```{r warning=FALSE, message=FALSE}

# Plot  proportion acumulada  de varianza explcada 

plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "b")
```


