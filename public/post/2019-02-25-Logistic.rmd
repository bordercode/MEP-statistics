---
title: "Logistic"
author: "JLMR"
date: 2019-02-28T21:14:14-05:00
categories: ["R"]
tags: ["Logistic", "log(odds)", "glm"]
mathjax : true
menu:
  main:
    name: Logistic R
    weight: 15
---


```{r set-global-options, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(eval = TRUE, 
                      echo = TRUE, 
                      cache = FALSE,
                      include = TRUE,
                      collapse = FALSE,
                      dependson = NULL,
                      engine = "R", # Chunks will always have R code, unless noted
                      error = FALSE)   


```



```{r silent-packages, echo = FALSE, eval = TRUE, message=FALSE, include = FALSE}
library(tidyverse)
library(ISLR)
library(foreign)
library(scales)
```



### Por qué el modelo de Regresión Logística?

En los casos anteriores hemos considerado que nuestros datos son númericos (variables contínuas o discretas pero de naturaleza cuantitativa).

Para el caso del **modelo de regresión logistica** nos interesa hacer predicciones para  variables **categóricas**.  

Reponder preguntas como: Cuál es la probabilidad de que un individuo partenezca a la categoría **i**? dado un conjunto de características ($x_i, ...,x_n$).

Ejemplo  1. Dado un conjunto de textos de Twitter, determinar cuales pertenecen a un sentimiento positivo y cuáles a un sentimiento negativo.

Ejemplo 2. Dadas las características de la madre, como edad, sexo, semanas de gestación, procedimiento cesárea o no, peso y talla del bebé determinar la probabilidad de que el bebé presente el padecimiento x. 

Ejemplo 3. Considere una muestra de individuos con sus perfiles de uso de tarjeta de crédito. Nos interesa predecir la probabilidad de que un individuo incumpla en los pagos de su tarjeta de crédito (incumplir =1 ,0 en caso contrario).

Ejemplo 4. Dadas las características de un conjunto de individuos (perfiles biométricos) determinar la probabilidad de que un individuo presente un padecimiento cardiáco.


El modelo de **regresión logística** nos permite **clasificar**. Predecir una respuesta cualitativa ej. asignar a una categoria determinada las observaciones en un conjunto de datos se puede realizar mediante el modelo  de regressión logística.

El proceso de clasificación implica determinar la **probabilidad** de que la observación *i* pretenezca a una determinada categoría. Lo que se puede lograr a partir del modelo de regresión logística.

**Escenario 1**

Dado el nivel de ingreso (variable *income*) y el saldo de la targeta de crédito (variable *balance*) en una muestra de 10,000 individuos incluidos estudiantes, ¿Cuál es la probabilidad de que un individuo de la muestra, incumpla el pago de su deuda? (*Default*). 

El primer paso para aproximar la solución de este problema de clasificación es representar las variables de interés. 

Note que la variable dependiente (**Y**) es *Default* y las explicativas (**x_1,...,X_n**) son *income* y *balance*.

**Scatter plot** entre las variables independientes (income y balance) para observar su interacción. 

```{r, echo=FALSE, message=FALSE, warning=FALSE}
d<-Default
str(d)
names(d)

p<-ggplot(d,aes(balance, income, colour=default, shape=default))+
  geom_point()+
  theme_light()+
  scale_shape_manual(values=c(1,3))+
  scale_colour_manual(values = c("orange", "darkgreen"))+
  scale_x_continuous(labels = dollar_format())+
  scale_y_continuous(labels = dollar_format())

p
```

Distribución de los datos con un **Boxplot**.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(gridExtra)

bxp1<-ggplot(d,aes(default, balance))+
  geom_boxplot(aes(fill=default))+
  theme_light()+
  scale_y_continuous(labels = dollar_format())+
  scale_fill_manual(values = c("orange", "darkgreen"))+
  theme(legend.position = "none")


bxp2<-ggplot(d,aes(default, income))+
  geom_boxplot(aes(fill=default))+
  theme_light()+
  scale_y_continuous(labels = dollar_format())+
  scale_fill_manual(values = c("orange", "darkgreen"))+
  theme(legend.position = "none")

grid.arrange(bxp1, bxp2, nrow = 1)

```


Note la relación positiva entre las variables saldo e incumplimiento. Así mismo una relación inversa respecto al ingreso, aparentemente aquellos individuos de menor ingreso tiene mayor problema de incumplimiento. 


##### Preeliminares. Contraste de regresión logística respecto a regresión linear con OLS. 



**Escenario 2**

Suponga que tenemos una situación en la que debemos clasificar el padecimiento de un  paciente que llega a la sala de emergencias de un hospital en las categorias: sobredosis o paro cardiáco

\begin{equation}
  Y=\left\{
  \begin{array}{@{}ll@{}}
    0, & \text{si}\ paro cardiáco \\
    1, & \text{sobredosis}
  \end{array}\right.
\end{equation} 


En este caso particular **binario** podríamos estimar un modelo **RL** con **MCO**  y considerar sobredosis si
$\hat{Y}>.5$ 

Sin embargo, nuestros estimados de **MCO** no necesariamente se ubicarian en el intervalo [0,1] de hecho es psible probar que que se pueden predecir $p(x)<0$ para algunos valores de x así como $p(x)>1$ para otros valores. Por lo tanto su interpretación como probabilidades no tiene sentido.

Aún más, el enfoque de **MCO** para variables cualitativas en un caso no binario, ej. 3 o 4 categorías, carece de sentido totalmente, ya que al cambiar el orden de la variable dependiente en el modelo lineal generariamos resultados diferentes y predicciones inconsistentes. 

Cuando estimamos el modelo de regresión logística lo que importa es el significado de cada categoría pero su orden no tiene mayor implicación.



#### Modelo de Regressión logística.

Este enfoque está diseñado explicitamente para tratar con variables categóricas (factores en el lenguaje de R, Python, etc,.). 

En este caso podemos estimar la probabilidad de que  **Y** pertenzca a una categoría particular.


Antes de avanzar veamos un par de conceptos preeliminares. Momios y ratio de Momios. 


Qué son los **Momios** (chances de que algo ocurra,  un ratio o cociente)(**odds**)?  Suponga que tenemos un escenario en el que un equipo participa en 5 partidos.  Gana uno y pierde 4 en este caso podemos calcular los **Odds** de ganar como el cociente:$$1/4=.25$$

En palabras tenemos un chance de ganar (odds) de 0.25.  

**Note que Odds no son probabilidades.** pero indican el cociente entre que suceda algo respecto a que no suceda.

Para llegar a estimar la probilidades que  un evento ocurra dado un conjunto de variables explicativas el modelo de regresión logística considera una serie de supuestos adicionales, en especial es importante incorporar la función logit, que veremos enseguida. 

Las probabiliadades son la tasa (**ratio**) de que algo ocurra respecto a **todo** lo que podria suceder. 
En el caso previo tenemos un total de 5 juegos así que la probabilid de  que mi equipo gane es: $$1/5=.20$$

Nuevamente suponga ahora que nuestro equipo participa en un total de  8 partidos. Gana 5 pierde 3. Igualmente podemos calcular los momios de ganar como $$5/3=1.7$$ Pero la probabilidad de ganar es: $$5/8=0.625$$

Note que la probabilidad de que mi equipo gane es **diferente** a los **Odds** (Momios) de que mi equipo gane.  


Ahora bien calculemos ahora la probabilidad de que el equipo pierda en el ejemplo anterior la probabilidad de que el equipo gane es 5/8, entonces  la probabilidad de perder es: $$ 3/8=0.375$$  o bien **1-prob** de ganar=1-0.625=0.375

Ahora note que podemos calcular los **momios** (Odds) de ganar  como el ratio de las probabilidades esto es: 

$$\frac{5/8}{1-5/8}=\frac{5/8}{3/8}=5/3=1.7$$

Podemos sintetizar la discusión anterior en la expresión. $$Odds=
\frac{p}{1-p}$$

**Log Odds** ¿Por qué considerar el logaritmo de los Momios (**log Odds**)? 

Considermos nuestro ejemplo inicial. Nuestro equipo juega 5 partidos, gana 1. Entonces tiene Odds de ganar de $$1/4=0.25$$ de forma similar: 

Si juega 9 partidos y gana 1, los Odds de ganar son: $$1/8=0.125$$
Si juega 17 partidos y gana 1, los Odds de ganar son: $$1/16=0.062$$
Si juega 33 partidos y gana 1, los Odds de ganar son: $$1/32=0.031$$

**Note** que si mi equipo es malo, los momios de ganar tienden a cero a medida que le número de partidos aumenta (a medida que el número de partidos tiende a inifnito ($\infty$)),  . de hecho si los Odds estan **encontra** de que mi equipo gane los momios se ubican en el intervalo cerrado [0,1]. Cero si pierde todos los partidos jugados, 1 si gana el mismo número de los que pierde.

Ahora observemos qué sucede con los momios  si estos estan a favor de ganar. 

Si juega 9 partidos y gana 6  $$6/3=2$$
Si juega 17 partidos y gana 14 $$14/3=4.6$$
Si juega 33 partidos y gana 30 $$30/3=10$$

Note que los momios a favor de ganar se incrementan a media que el número de juegos aumenta y de hecho  tienden a $\infty$. Pero esta vez se ubican en el intervalo $[1,\infty)$.  Uno si al menos gana los mismos partidos de los que pierde y tiende a $\infty$ si gana todos los juegos jugados $n/0$ (no esta definido pero tiende a **infinito**).

Note que entre los momios a favor de perder  y los de ganar se tiene una NOTABLE asimetría.  Mientras los primeros se ubican en $[0,1]$, los segundos se ubican en el intervalo $[1,\infty)$. Una diferencia abismal en la magnitud !!!

Por ejemplo  Si tenemos 7 juegos. Los momios en contra por ejemplo son $1/6=.16$, pero los momios a favor son: $6/1=6$. Una magnitud completamente desproporcionada!!!

A pesar de que en cada caso sólo se gana o se pierde un sólo juego. 

Este problema de asimetría en los momios se resuelve tomando el logaritmo de los momios.

Ejemplo: $$log(1/6)=-1.791759$$ $$log(6/1)=1.791759$$ 

Ahora la magnitud en los momios es la misma, es **simetrica**!!

**El log del ratio de probabilidades e.j. log(Odds) Se denomina la función  logit**. $$LOGIT=log(\frac{p}{1-p})$$ 

**Importante** esta expresión tiene como distribución asociada  la clásica standard (NORMAL) y por lo tanto estimar probabilides como áreas debajo de la curva de esta distribución resulta pertinente.




Un punto adicional, los Odds son un ratio un cociente entre dos eventos (las veces que ocurre un resultado entre las veces que **no** ocurre ese resultado).

A pesar de que los **momios** se calculan como un ratio, no son lo mismo a cuando nos referiemos a el **ratio de momios**. En este segundo caso estamos hablando de un cociente entre dos momios. ej. Momio 1: $8/5$, Momio 2: $3/7$ ratio de momios 1 y 2= $\frac{8/5}{3/7}=3.73$


##Función logit. derivación:  


Sea $$p(X)=Pr(Y=1|X)$$ y considerando una representación lineal  $$p(X)=\beta_0 +\beta_1 X$$

Aplicando la definicón del log Odds, tenemos: 
$$log\bigg(\frac{P(X)}{1-P(X)}\bigg)= \beta_0 +\beta_1 X$$
Donde el lado izquierdo es nombrado función **logit** o log Odds.

$$\frac{P(X)}{1-P(X)}=e^{ \beta_0 +\beta_1 X}$$


Note que eliminamos el logaritmo aplicando la operación inversa el *e* para ambos lados de la ecuación. En palabras, la estimación de los momios es igual a la exponencial de la ecuación lineal $\beta_0+\beta_1X$.

El lado izquierdo es el ratio de las probabilidades o de manera equivalente los **momios** (Odds). 


Despejando  p(x), tenemos el **modelo de regresión  logística**
$$p(X)=\frac{e^{ \beta_0 +\beta_1 X}}{1 + e^{ \beta_0 +\beta_1 X}}$$
Que de hecho incorpora la **función logit** y es lineal en x.



![](/img/kogplt.jpg)

La estimación de los **coeficientes del modelo que haremos** genera precisamente el valor de **logaritmo del ratio de momios** (momios entre dos variables, ej. ratio de momios de ser admitido a cursar un doctorado con beca en **UCLA** para Mujeres vs. hombres) **log(odds ratio)**, no de la **probabilidad**. 

La probabilidad se obtiene a partir de substituir los coeficientes en la función de regresión logistica para valores específicos de las variables independientes **x**.

##### Estimación de coeficientes en el modelo de regresión logistica. 

Usamos el procedimiento de **Máxima verosimilitud** a diferencia del caso del modelo de regressión lineal en donde se aplicó  **MCO**

Este permite seleccionar los coeficientes $\beta_0 , \beta_1$ que maximizan la **función de verosimilitud**. 

$$\ell ({\beta_0, \beta_1})=\prod_{i:y_{i}=1}p(x_i)\prod_{i':y_{i'}=0}(1-p(x_{i'}))$$
En palabras, la función de máxima verosimilitud  realiza un proceso interativo para seleccionar la linea que maximiza el logaritmo de la función de verosimilitud ´-log(likelihood)- calculando  el producto de  cada probabilidad de que el evento ocurra por el producto de las probabilidades estimadas de que el evento no odurra. 

**Maximizar la función de verosimilitud** quiere decir determinar aquella  probabilidad $\hat{p}(x_i)$ que es lo más apegada posible al valor observado de los datos. 

En nuestro ejemplo con la base de datos sobre incumplimiento (*default*) del pago en tarjeta de crédito, buscamos los valores de $\hat{\beta_0}, \hat{\beta_1}$ que al substituirlos en la función $p(X)$, resulten en un número lo más cercano posible a 1 para los individuos en la catergoría default y cercano a cero en caso contrario. 

##### Prueba para determinar si el log(odds) es estadísticamente significativo. 

Usamos la prueba de Wald. (**Wald´s test**).

Con esta obtenemos los intervalos de confianza así como los p-values para determinar la significancia estadística de los coeficientes. 

Importante recordar que el logaritmo del cociente de  momios log(odd ratios), se distribuyen acorde a la **normal** con:   $N\approx(\mu=0,\sigma=1)$ y recordando los fundamentos del **Teorema de limite Central**  podemos estimar **intervalos de confianza** y  *p values* como areas de bajo de la curva de la distribución. 

```{r, echo=FALSE}


library(gridExtra)

set.seed(3000)
xseq<-seq(-4,4.01)
densities<-dnorm(xseq, 0,1)
cumulative<-pnorm(xseq, 0, 1)
randomdeviates<-rnorm(1000,0,1)
 
par(mfrow=c(1,3), mar=c(3,4,4,2))

plot(xseq, densities, col="darkgreen",xlab="", ylab="Densidad", type="l",lwd=2, cex=2, main="Función Densidad de Probabilidades PDF", cex.axis=.8)

plot(xseq, cumulative, col="darkorange", xlab="", ylab="Probabilidad Acumulada",type="l",lwd=2, cex=2, main="CDF de Standard Normal", cex.axis=.8)

hist(randomdeviates, main="Selección Aleatoria de Std Normal", cex.axis=.8, xlim=c(-4,4))
```
[Source:](https://www.r-bloggers.com/normal-distribution-functions/) 

Regresando a la estimación de la prueba de **Wald** para determinar la significancia estadística de los coeficientes obtenidos en el modelo de regresión logística  (log(odd) y log(odds ratios) observe  que la hipótesis nula $H_0: \hat{\beta_1}=0$ lo que implica que:

$$p(X)=\frac{e^{\beta_0}}{1+e^{\beta_0}}$$
O bien que las variables independientes no tienen impacto o son irrelevantes para explicar la variable dependiente.

Tal como hacemos en cualquier prueba de hipótesis, rechazamos la **hipótesis nula** si el $p-value<\alpha$ threshold de contraste, usualmente 0.05.

El valor del intercepto en el modelo permite el ajuste de las probabilidades a la proporción observada de valores positivos en los datos (ej. Y=1)

#### McFadden's Pseudo R^2. Prueba para determinar si el modelo es útil.

Ahora como analogía de los modelos de RL MCO, es posible estimar una medida que nos indique si el modelo de **R-Logistica** es válido. Este ese el indicador Pseudo $R^2$ de McFadden.
$$R^2=\frac{ll(Nulldeviance )-ll(Residualdeviance)}{ll(Nulldeviance)}$$
Donde:
$$ll(Residualdeviance)=log(likelyhood) max$$
$$ll(null)=overall prob=log(likelyhood|1)$$

####Ejemplo 1 Modelo de regressión logística.


```{r, echo=FALSE}
logistic<-glm(default~balance, data=d,family=binomial)

summary(logistic)
```
Un incremento de una unidad en la variable **Balance** se relaciona con  un incremento de .00549 en los **log(odds)** de la variable *default* y esta relación es estadísticamente significativa al 0.05. 

Hacemos ahora predicciones de probabilidades con base en el logartimo de momios estimados log(odds).  Por ejemplo para un individuo cuyo balance=$\$1,000$
  
$$\hat {p}(X)=\frac{e^{ \hat{\beta_0} +\hat{\beta_1} X}}{1 + e^{\hat{ \beta_0} +\hat{\beta_1 }X}}=\frac{e^{−10.6513+0.0055×1,000}}{1+e^{−10.6513+0.0055×1,000}}=0.00576$$
Una probabilidad de default de $0.57\%$. 

Ahora hacemos la predicción para un  individuo con un saldo (*balance*) de $\$2,000$.

$$\hat {p}(X)=\frac{e^{ \hat{\beta_0} +\hat{\beta_1} X}}{1 + e^{\hat{ \beta_0} +\hat{\beta_1 }X}}=\frac{e^{−10.6513+0.0055×2,000}}{1+e^{−10.6513+0.0055×2,000}}=0.586$$
Un individuo con un balance de 2000 en su tarjeta tiene una probabilidad de incumplimiento (*default*) $58.6\%$


Estimamos el $R^2$ como :

```{r, echo=TRUE}
## Calculamos el  "Pseudo R-squared" y su  p-value

Null_deviance <- logistic$null.deviance
Residual_deviance <- logistic$deviance

## Donde Null_deviance es el loglikelihood del modelo nulo (modelo solo con el intercepto)

## McFadden's Pseudo R^2 = (Null_deviance - Residual_deviance) / Null_deviance
(Null_deviance - Residual_deviance) / Null_deviance


## Si el modelo es útil el r^2 esta mas cercano a 1, (no es común).
#Valores aprox .40 son comunes en sc.


## Especificación para el p-value con distribución chi cuadrada.

## El p-value para el  R^2 a partir de la distribución chi cuadrada.

1 - pchisq(2*(Null_deviance-Residual_deviance ),df=(length(logistic$coefficients)-1))

## df Coeficientes menos 1. en este caso 2 coef -1, df=1. 
```

El $R^2$ es estadísticamente significativo,  el modelo es útil.


Graficamos la distribución de probabilidades estimada por el modelo.

```{r, echo=FALSE}
pred<- data.frame(
  pr=logistic$fitted.values,
  default=d$default, b=d$balance)

pred<-arrange(pred,pr)%>%
  mutate(id=1:nrow(pred))
glimpse(pred)
```


```{r, echo=TRUE}

ggplot(pred, aes(b, pr)) +
  geom_point(aes(color=default), 
  alpha=1, shape=21) +
    xlab("Balance") +
  ylab("Pr(de incumplimiento)")+
  theme_classic()+
  scale_color_manual(values =c("darkgreen","orange"))+
  labs(color="Default")+
  geom_hline(yintercept=0,linetype="dashed", size=.5, , color="blue")+
   geom_hline(yintercept=1,linetype="dashed", size=.5, color="red")
```




##### Ejemplo 2. Estimación de modelo de regresión logística con una variable dicotómica.

Considere el escenario de la variable dicotomica *student*. que toma los valores 1 si el individuo es estudiante y 0 en caso contrario. Estimaremos el modelo  para la variable dependiente **Default** y ahora con la variable independiente dicotomica  **student**. 

```{r}
l2<-glm(default~student, data=d, family=binomial)
summary(l2)
```
El resultado indica que de hecho el ser estudiante incrementa el  **log(odds)** de incumplimiento en .40. (**NO LA PROBABILID pero el log(odds)**).


Note que el estadístico **z** tiene el mismo papel que el estadístico **t** en el modelo de **RL** Y se estima a partir del cociente $\frac{\hat {\beta_1}}{SE\hat{\beta_1}}$



Ahora podemos estimar las probabilidades de incumplimiento para un individuo particular usando el coeficiente estimado y la ecuación del modelo de regresión logística que ya conocemos.

$$\hat{pr} (D=si|E=si)=\frac{e^{−3.5041+0.4049×1}}{1 + e^{−3.5041+0.4049×1}}=0.0431$$

$$\hat{pr}(D=si|E=no)=\frac{e^{−3.5041}}{1 + e^{−3.5041}}=0.0292$$

La probabilidad de incumplimiento para un estudiante es de $4.31\%$ minetras que para individuo no estudiante es $2.92\%$.


